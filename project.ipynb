{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import numpy as np\n",
    "from neuralNetwork import NeuralNetwork, In_between_epochs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import shuffle, randint\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoTokenizer, AutoModel\n",
    "from helper import dict_lists_to_list_of_dicts, get_train_valdiation_test_split, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ECHR_Corpus.json\",encoding= 'utf-8')\n",
    "dataset = load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "ogni caso nel dataset è organizzato così:\n",
    "```json\n",
    "{\n",
    "    \"text\":\"il testo completo della sentenza\"\n",
    "    \"clauses\":[\n",
    "        {\n",
    "            \"_id\": \"id della clause\",\n",
    "            \"start\": \"index di inizio della clause\",\n",
    "            \"end\": \"index di fine della cluause\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "    \"argument\":[\n",
    "        {\n",
    "            \"premises\":[\n",
    "                \"id premise1\", \"id premise2\", \"...\"\n",
    "            ],\n",
    "            \"conclusion\": \"id conclusion\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "In questo passaggio riorganizzo i dati per avere comunque la stessa struttura ma trasformo le clauses in un dizionario che ha come id la chiave della clause e come valore il testo (senza quindi dover usare start e end per cercarlo nel testo). Non tutte le clause sono parte di una premessa o di una conclusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_dataset = []\n",
    "for datapoint in dataset:\n",
    "    text = datapoint[\"text\"]\n",
    "    dict_clauses = {}\n",
    "    for clause in datapoint[\"clauses\"]:\n",
    "        start = clause[\"start\"]\n",
    "        end = clause[\"end\"]\n",
    "        id = clause[\"_id\"]\n",
    "        dict_clauses[id] = text[start:end]\n",
    "    refactored_dataset.append({\n",
    "        \"text\": text,\n",
    "        \"arguments\": datapoint[\"arguments\"],\n",
    "        \"n_clauses\": len(datapoint[\"clauses\"]),\n",
    "        \"all_clauses\": dict_clauses\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondo me ha poco senso salvare il dataset come dataframe dato che è praticamente solo testo. Comunque non dovrebbe essere difficilissimo tirarci fuori qualche statistica. Ho fatto degli esempi scemi qua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On average, a case has 248.95 clauses with a median of 226 clauses per case.\n",
      "On average, a case has 17.69 arguments with a median of 14 arguments per case.\n",
      "Each argument, on average, has: 2.63 premises with a median of 2 premises per argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_arguments = []\n",
    "n_premises = []\n",
    "n_clauses = []\n",
    "for case in refactored_dataset:\n",
    "    n_arguments.append(len(case[\"arguments\"]))\n",
    "    n_clauses.append(case[\"n_clauses\"])\n",
    "    for argument in case[\"arguments\"]:\n",
    "        n_premises.append(len(argument[\"premises\"]))\n",
    "print(f\"\"\"\n",
    "On average, a case has {np.mean(n_clauses):.2f} clauses with a median of {np.median(n_clauses):.0f} clauses per case.\n",
    "On average, a case has {np.mean(n_arguments):.2f} arguments with a median of {np.median(n_arguments):.0f} arguments per case.\n",
    "Each argument, on average, has: {np.mean(n_premises):.2f} premises with a median of {np.median(n_premises):.0f} premises per argument.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qua puoi vedere come tirare fuori il testo di una conclusion o di una premise: ti basta usare la conclusion/premise come indice nel dizionario delle clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here an example of an argument:\n",
      "    - Premises:\n",
      "        The Commission notes that the applicant was detained after having been sentenced by the first instance court to 18 months' imprisonment.\n",
      "\tHe was released after the Court of Appeal reviewed this sentence, reducing it to 15 months' imprisonment, convertible to a fine.\n",
      "    - Conclusion:\n",
      "        The Commission finds that the applicant was deprived of his liberty \"after conviction by a competent court\" within the meaning of Article 5 para. 1 (a) (Art. 5-1-a) of the Convention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premise = \"\\n\\t\".join([refactored_dataset[0][\"all_clauses\"][premise] for premise in refactored_dataset[0][\"arguments\"][0][\"premises\"]])\n",
    "conclusion = refactored_dataset[0][\"all_clauses\"][refactored_dataset[0][\"arguments\"][0][\"conclusion\"]]\n",
    "print(f\"\"\"\n",
    "Here an example of an argument:\n",
    "    - Premises:\n",
    "        {premise}\n",
    "    - Conclusion:\n",
    "        {conclusion}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Argument Clause Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = get_train_valdiation_test_split(refactored_dataset, [4])#, seed = 42) #42, 4, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 28 cases in the training set (fold 0), \n",
      "    6 cases in the validation set (fold 0) and\n",
      "    8 cases in the test set (fold 0)\n",
      "    \n",
      "there are 28 cases in the training set (fold 1), \n",
      "    6 cases in the validation set (fold 1) and\n",
      "    8 cases in the test set (fold 1)\n",
      "    \n",
      "there are 28 cases in the training set (fold 2), \n",
      "    6 cases in the validation set (fold 2) and\n",
      "    8 cases in the test set (fold 2)\n",
      "    \n",
      "there are 28 cases in the training set (fold 3), \n",
      "    6 cases in the validation set (fold 3) and\n",
      "    8 cases in the test set (fold 3)\n",
      "    \n",
      "there are 28 cases in the training set (fold 4), \n",
      "    6 cases in the validation set (fold 4) and\n",
      "    8 cases in the test set (fold 4)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cases)):\n",
    "    train_cases, validation_cases, test_cases = cases[i]\n",
    "    print(f\"\"\"there are {len(train_cases)} cases in the training set (fold {i}), \n",
    "    {len(validation_cases)} cases in the validation set (fold {i}) and\n",
    "    {len(test_cases)} cases in the test set (fold {i})\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dato che non tutte le clause sono parte di una premise/conclusion, le dividiamo per cercare di tirare fuori un classificatore di clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "\n",
      "    There are:\n",
      "        - 3355 clause\n",
      "        - 1846 true clause\n",
      "        - 1509 fake clause\n",
      "    \n",
      "validation set:\n",
      "\n",
      "    There are:\n",
      "        - 288 clause\n",
      "        - 223 true clause\n",
      "        - 65 fake clause\n",
      "    \n",
      "test set:\n",
      "\n",
      "    There are:\n",
      "        - 1046 clause\n",
      "        - 624 true clause\n",
      "        - 422 fake clause\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def get_acr_dataloader(cases, tokenizer, batch_size=16, shuffle=False, verbose=True):\n",
    "    ACR_x = []\n",
    "    ACR_y = []\n",
    "    for case in cases:\n",
    "        n_clauses = case[\"n_clauses\"]\n",
    "        clauses = case[\"all_clauses\"]\n",
    "        args_set = set()\n",
    "        splitter = \"AS TO THE LAW\" if \"AS TO THE LAW\" in case[\"text\"] else \"THE LAW\"\n",
    "        law_section = case[\"text\"].split(splitter)[1]\n",
    "        for argument in case[\"arguments\"]:\n",
    "            if not argument[\"conclusion\"] in args_set and clauses[argument[\"conclusion\"]] in law_section:\n",
    "                ACR_x.append(clauses[argument[\"conclusion\"]])\n",
    "                ACR_y.append(torch.tensor([1.,0.]))\n",
    "                args_set.add(argument[\"conclusion\"])\n",
    "            for premise in argument[\"premises\"]:\n",
    "                if not premise in args_set and clauses[premise] in law_section:\n",
    "                    ACR_x.append(clauses[premise])\n",
    "                    ACR_y.append(torch.tensor([1.,0.]))\n",
    "                    args_set.add(premise)\n",
    "        for clause_id in clauses.keys():\n",
    "            if not clause_id in args_set and clauses[clause_id] in law_section:\n",
    "                ACR_x.append(clauses[clause_id])\n",
    "                ACR_y.append(torch.tensor([0.,1.]))\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "    There are:\n",
    "        - {len(ACR_x)} clause\n",
    "        - {len([a for a in ACR_y if a[0] == 1])} true clause\n",
    "        - {len([a for a in ACR_y if a[0] == 0])} fake clause\n",
    "    \"\"\")\n",
    "    ACR_x_tokenized = dict_lists_to_list_of_dicts(tokenizer(ACR_x, padding=True, truncation=True, return_tensors='pt'))\n",
    "    dataset = Dataset(ACR_x_tokenized, ACR_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "train_cases, validation_cases, test_cases = cases[-1]\n",
    "print(\"train set:\")\n",
    "train_dataloader = get_acr_dataloader(train_cases, tokenizer, shuffle=True, batch_size=8)\n",
    "print(\"validation set:\")\n",
    "validation_dataloder = get_acr_dataloader(validation_cases, tokenizer, batch_size=8)\n",
    "print(\"test set:\")\n",
    "test_dataloader = get_acr_dataloader(test_cases, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Da qui in avanti ti dovrebbe essere tutto abbastanza familiare dato che è praticamente lo stesso codice dell'assignment 2. Qua ho fatto solo 5 epoche e con un learning rate a caso ma è giusto per far vedere come funziona. Da ora si può iniziare a giocare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoded_input, _ = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "        encoded_input = torch.nn.functional.avg_pool1d(\n",
    "            encoded_input.permute(0, 2, 1), \n",
    "            kernel_size=encoded_input.size(1)\n",
    "        ).squeeze(2)\n",
    "        if len(encoded_input.size()) != 2:\n",
    "            print(encoded_input.size())\n",
    "        return self.output_layer(encoded_input).float()\n",
    "\n",
    "    def freeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    def unfreeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "model = Model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operating on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"operating on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(In_between_epochs):\n",
    "    def __init__(self, delta, patience):\n",
    "        self.delta = delta\n",
    "        self.patience = patience\n",
    "        self.current_patience = 0\n",
    "        self.best_valid_loss = 100000000000000000000\n",
    "        self.best_model = Model(2)\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def __call__(self, model:torch.nn.Module, loaders:dict[str,torch.utils.data.DataLoader], device:'torch.device|str', output_extraction_function, losses:dict[str, float]) -> bool:\n",
    "        self.epochs += 1\n",
    "        if losses[\"validation\"] < self.best_valid_loss - self.delta:\n",
    "            self.best_valid_loss = losses[\"validation\"]\n",
    "            self.best_model.load_state_dict(model.state_dict())\n",
    "            self.current_patience = 0\n",
    "        else:\n",
    "            self.current_patience += 1\n",
    "            if self.current_patience >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    def reset(self):\n",
    "        self.current_patience = 0\n",
    "        self.epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train, validation, min_lr, start_lr, early_stopping, frac):\n",
    "    tot_train_data, tot_val_data = [], []\n",
    "    lr = start_lr\n",
    "    while lr > min_lr:\n",
    "        train_data, validation_data = model.train_network(train, \n",
    "                        validation, \n",
    "                        torch.optim.Adam, \n",
    "                        loss_function=nn.CrossEntropyLoss(),\n",
    "                        device=device, \n",
    "                        batch_size=32,\n",
    "                        verbose=True, \n",
    "                        output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                        metrics={\n",
    "                        \"accuracy\": accuracy_score, \n",
    "                        \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                        in_between_epochs = {\"early_stopping\": early_stopping},\n",
    "                        learning_rate=lr,\n",
    "                        epochs=30)\n",
    "        train_data[\"epochs\"] = early_stopping.epochs\n",
    "        train_data[\"lr\"] = lr\n",
    "        validation_data[\"epochs\"] = early_stopping.epochs\n",
    "        validation_data[\"lr\"] = lr\n",
    "        tot_train_data.append(train_data)\n",
    "        tot_val_data.append(validation_data)\n",
    "        model.load_state_dict(early_stopping.best_model.state_dict())\n",
    "        lr = lr * frac\n",
    "        early_stopping.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_train(lrs:list[float], train, validation):\n",
    "    trainings = []\n",
    "    models = []\n",
    "    for lr in lrs:\n",
    "        model = Model(2)\n",
    "        train_data, validation_data = model.train_network(train, \n",
    "                        validation, \n",
    "                        torch.optim.Adam, \n",
    "                        loss_function=nn.CrossEntropyLoss(),\n",
    "                        device=device, \n",
    "                        batch_size=32,\n",
    "                        verbose=True, \n",
    "                        output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                        metrics={\n",
    "                        \"accuracy\": accuracy_score, \n",
    "                        \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                        learning_rate=lr,\n",
    "                        epochs=15)\n",
    "        for key in train_data:\n",
    "                train_data[key] = [float(v) for v in train_data[key]]\n",
    "                validation_data[key] = [float(v) for v in validation_data[key]]\n",
    "        trainings.append({\"train\": train_data, \"validation\":validation_data, \"lr\":lr})\n",
    "        models.append(model)\n",
    "    return trainings, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_training(cases):\n",
    "    best_models = []\n",
    "    for cases_split in cases:\n",
    "        train_cases, validation_cases, _ = cases_split\n",
    "        train_dataloader = get_acr_dataloader(train_cases, tokenizer, shuffle=True, batch_size=8, verbose=False)\n",
    "        validation_dataloder = get_acr_dataloader(validation_cases, tokenizer, batch_size=8, verbose=False)\n",
    "        model = Model(2)\n",
    "        early_stopping = EarlyStopping(.001, 3)\n",
    "        train(model, train_dataloader, validation_dataloder, 5e-7, 1e-6, early_stopping, .6)\n",
    "        model.load_state_dict(early_stopping.best_model.state_dict())\n",
    "        best_models.append({\"loss\": early_stopping.best_valid_loss, \"model\": model})\n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.606 - validation loss:      0.729                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.699 - validation accuracy:      0.490\n",
      "EPOCH 1 training f1_score:      0.651 - validation f1_score:      0.350\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.495 - validation loss:      0.701                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.776 - validation accuracy:      0.621\n",
      "EPOCH 2 training f1_score:      0.735 - validation f1_score:      0.449\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.441 - validation loss:      0.720                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.802 - validation accuracy:      0.641\n",
      "EPOCH 3 training f1_score:      0.771 - validation f1_score:      0.468\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.398 - validation loss:      0.734                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.832 - validation accuracy:      0.628\n",
      "EPOCH 4 training f1_score:      0.808 - validation f1_score:      0.460\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.358 - validation loss:      0.684                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.849 - validation accuracy:      0.647\n",
      "EPOCH 5 training f1_score:      0.814 - validation f1_score:      0.480\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 6 training loss:      0.318 - validation loss:      0.705                                                                                                                                                                                                                                                      \n",
      "EPOCH 6 training accuracy:      0.870 - validation accuracy:      0.670\n",
      "EPOCH 6 training f1_score:      0.844 - validation f1_score:      0.498\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 7 training loss:      0.268 - validation loss:      0.778                                                                                                                                                                                                                                                      \n",
      "EPOCH 7 training accuracy:      0.895 - validation accuracy:      0.641\n",
      "EPOCH 7 training f1_score:      0.875 - validation f1_score:      0.475\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 8 training loss:      0.229 - validation loss:      0.833                                                                                                                                                                                                                                                      \n",
      "EPOCH 8 training accuracy:      0.916 - validation accuracy:      0.641\n",
      "EPOCH 8 training f1_score:      0.903 - validation f1_score:      0.465\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 8 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.328 - validation loss:      0.717                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.862 - validation accuracy:      0.639\n",
      "EPOCH 1 training f1_score:      0.837 - validation f1_score:      0.474\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.302 - validation loss:      0.772                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.884 - validation accuracy:      0.615\n",
      "EPOCH 2 training f1_score:      0.863 - validation f1_score:      0.447\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.278 - validation loss:      0.751                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.887 - validation accuracy:      0.636\n",
      "EPOCH 3 training f1_score:      0.863 - validation f1_score:      0.464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 3 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.622 - validation loss:      0.683                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.677 - validation accuracy:      0.564\n",
      "EPOCH 1 training f1_score:      0.538 - validation f1_score:      0.522\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.522 - validation loss:      0.645                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.747 - validation accuracy:      0.643\n",
      "EPOCH 2 training f1_score:      0.666 - validation f1_score:      0.569\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.512 - validation loss:      0.771                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.754 - validation accuracy:      0.628\n",
      "EPOCH 3 training f1_score:      0.673 - validation f1_score:      0.554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.430 - validation loss:      0.686                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.800 - validation accuracy:      0.657\n",
      "EPOCH 4 training f1_score:      0.743 - validation f1_score:      0.557\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.379 - validation loss:      0.665                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.833 - validation accuracy:      0.683\n",
      "EPOCH 5 training f1_score:      0.790 - validation f1_score:      0.562\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 5 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.482 - validation loss:      0.641                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.772 - validation accuracy:      0.659\n",
      "EPOCH 1 training f1_score:      0.716 - validation f1_score:      0.537\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.455 - validation loss:      0.650                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.794 - validation accuracy:      0.667\n",
      "EPOCH 2 training f1_score:      0.744 - validation f1_score:      0.540\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.428 - validation loss:      0.656                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.807 - validation accuracy:      0.669\n",
      "EPOCH 3 training f1_score:      0.752 - validation f1_score:      0.547\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.412 - validation loss:      0.694                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.815 - validation accuracy:      0.666\n",
      "EPOCH 4 training f1_score:      0.765 - validation f1_score:      0.554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 4 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.611 - validation loss:      0.642                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.688 - validation accuracy:      0.640\n",
      "EPOCH 1 training f1_score:      0.586 - validation f1_score:      0.570\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.518 - validation loss:      0.567                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.744 - validation accuracy:      0.684\n",
      "EPOCH 2 training f1_score:      0.708 - validation f1_score:      0.524\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.449 - validation loss:      0.536                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.796 - validation accuracy:      0.717\n",
      "EPOCH 3 training f1_score:      0.761 - validation f1_score:      0.544\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.405 - validation loss:      0.528                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.821 - validation accuracy:      0.732\n",
      "EPOCH 4 training f1_score:      0.790 - validation f1_score:      0.578\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.364 - validation loss:      0.521                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.846 - validation accuracy:      0.744\n",
      "EPOCH 5 training f1_score:      0.818 - validation f1_score:      0.573\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 6 training loss:      0.344 - validation loss:      0.569                                                                                                                                                                                                                                                      \n",
      "EPOCH 6 training accuracy:      0.847 - validation accuracy:      0.729\n",
      "EPOCH 6 training f1_score:      0.813 - validation f1_score:      0.598\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 7 training loss:      0.300 - validation loss:      0.542                                                                                                                                                                                                                                                      \n",
      "EPOCH 7 training accuracy:      0.877 - validation accuracy:      0.727\n",
      "EPOCH 7 training f1_score:      0.856 - validation f1_score:      0.526\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 8 training loss:      0.242 - validation loss:      0.533                                                                                                                                                                                                                                                      \n",
      "EPOCH 8 training accuracy:      0.910 - validation accuracy:      0.741\n",
      "EPOCH 8 training f1_score:      0.886 - validation f1_score:      0.565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 8 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.337 - validation loss:      0.516                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.859 - validation accuracy:      0.735\n",
      "EPOCH 1 training f1_score:      0.836 - validation f1_score:      0.553\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.313 - validation loss:      0.523                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.873 - validation accuracy:      0.738\n",
      "EPOCH 2 training f1_score:      0.848 - validation f1_score:      0.550\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.292 - validation loss:      0.538                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.884 - validation accuracy:      0.735\n",
      "EPOCH 3 training f1_score:      0.863 - validation f1_score:      0.557\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.272 - validation loss:      0.537                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.891 - validation accuracy:      0.745\n",
      "EPOCH 4 training f1_score:      0.873 - validation f1_score:      0.562\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 4 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.635 - validation loss:      0.612                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.635 - validation accuracy:      0.718\n",
      "EPOCH 1 training f1_score:      0.491 - validation f1_score:      0.629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.528 - validation loss:      0.500                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.737 - validation accuracy:      0.807\n",
      "EPOCH 2 training f1_score:      0.683 - validation f1_score:      0.682\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.468 - validation loss:      0.472                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.767 - validation accuracy:      0.825\n",
      "EPOCH 3 training f1_score:      0.710 - validation f1_score:      0.669\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.424 - validation loss:      0.473                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.795 - validation accuracy:      0.815\n",
      "EPOCH 4 training f1_score:      0.748 - validation f1_score:      0.664\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.366 - validation loss:      0.544                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.849 - validation accuracy:      0.744\n",
      "EPOCH 5 training f1_score:      0.824 - validation f1_score:      0.582\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 6 training loss:      0.316 - validation loss:      0.505                                                                                                                                                                                                                                                      \n",
      "EPOCH 6 training accuracy:      0.873 - validation accuracy:      0.777\n",
      "EPOCH 6 training f1_score:      0.846 - validation f1_score:      0.606\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 6 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.431 - validation loss:      0.490                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.800 - validation accuracy:      0.795\n",
      "EPOCH 1 training f1_score:      0.759 - validation f1_score:      0.632\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.405 - validation loss:      0.484                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.812 - validation accuracy:      0.787\n",
      "EPOCH 2 training f1_score:      0.775 - validation f1_score:      0.604\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.378 - validation loss:      0.489                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.833 - validation accuracy:      0.786\n",
      "EPOCH 3 training f1_score:      0.796 - validation f1_score:      0.618\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 3 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.600 - validation loss:      0.606                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.704 - validation accuracy:      0.729\n",
      "EPOCH 1 training f1_score:      0.652 - validation f1_score:      0.551\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.509 - validation loss:      0.510                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.744 - validation accuracy:      0.781\n",
      "EPOCH 2 training f1_score:      0.698 - validation f1_score:      0.570\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.460 - validation loss:      0.464                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.785 - validation accuracy:      0.799\n",
      "EPOCH 3 training f1_score:      0.747 - validation f1_score:      0.652\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.413 - validation loss:      0.511                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.816 - validation accuracy:      0.771\n",
      "EPOCH 4 training f1_score:      0.789 - validation f1_score:      0.562\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.376 - validation loss:      0.541                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.840 - validation accuracy:      0.736\n",
      "EPOCH 5 training f1_score:      0.813 - validation f1_score:      0.538\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 6 training loss:      0.334 - validation loss:      0.482                                                                                                                                                                                                                                                      \n",
      "EPOCH 6 training accuracy:      0.864 - validation accuracy:      0.792\n",
      "EPOCH 6 training f1_score:      0.839 - validation f1_score:      0.612\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 6 epochs because of in between early_stopping\n",
      "EPOCH 1 training loss:      0.433 - validation loss:      0.473                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.801 - validation accuracy:      0.802\n",
      "EPOCH 1 training f1_score:      0.760 - validation f1_score:      0.653\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.406 - validation loss:      0.476                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.821 - validation accuracy:      0.816\n",
      "EPOCH 2 training f1_score:      0.790 - validation f1_score:      0.651\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.382 - validation loss:      0.508                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.832 - validation accuracy:      0.774\n",
      "EPOCH 3 training f1_score:      0.810 - validation f1_score:      0.570\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "stopping after 3 epochs because of in between early_stopping\n"
     ]
    }
   ],
   "source": [
    "models = fold_training(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loss: 0.46365880966186523\n"
     ]
    }
   ],
   "source": [
    "best_model_fold = min(models, key= lambda x: x[\"loss\"]) \n",
    "print(f\"best model loss: {best_model_fold['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = Model(2)\n",
    "best_model.load_state_dict(best_model_fold[\"model\"].state_dict())\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.95\n",
      "    f1: 0.97\n",
      "    precision: 0.98\n",
      "    recall: 0.96\n"
     ]
    }
   ],
   "source": [
    "def full_test(model, cases):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for case in cases:\n",
    "            clauses = case[\"all_clauses\"]\n",
    "            args_set = set()\n",
    "            splitter = \"AS TO THE LAW\" if \"AS TO THE LAW\" in case[\"text\"] else \"THE LAW\"\n",
    "            law_section = case[\"text\"].split(splitter)[1]\n",
    "            for argument in case[\"arguments\"]:\n",
    "                if argument[\"conclusion\"] not in args_set and clauses[argument[\"conclusion\"]] in law_section:\n",
    "                    tokenized_x = tokenizer(clauses[argument[\"conclusion\"]], truncation=True, return_tensors='pt')\n",
    "                    tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                    pred = model(tokenized_x)\n",
    "                    y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                    y_true.append(0)\n",
    "                    keys = list(tokenized_x.keys())\n",
    "                    for k in keys:\n",
    "                        del tokenized_x[k]\n",
    "                    del tokenized_x\n",
    "                    args_set.add(argument[\"conclusion\"])\n",
    "                elif argument[\"conclusion\"] not in args_set and clauses[argument[\"conclusion\"]] in law_section:\n",
    "                    y_pred.append(1)\n",
    "                    y_true.append(0)\n",
    "                for premise in argument[\"premises\"]:\n",
    "                    if premise not in args_set and clauses[premise] in law_section:\n",
    "                        tokenized_x = tokenizer(clauses[premise], truncation=True, return_tensors='pt')\n",
    "                        tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                        pred = model(tokenized_x)\n",
    "                        y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                        y_true.append(0)\n",
    "                        keys = list(tokenized_x.keys())\n",
    "                        for k in keys:\n",
    "                            del tokenized_x[k]\n",
    "                        del tokenized_x\n",
    "                        args_set.add(premise)\n",
    "                    elif premise not in args_set and clauses[premise] in law_section:\n",
    "                        y_pred.append(1)\n",
    "                        y_true.append(0)\n",
    "            for clause_id in clauses.keys():\n",
    "                if not clause_id in args_set and clauses[clause_id] in law_section:\n",
    "                    tokenized_x = tokenizer(clauses[clause_id], truncation=True, return_tensors='pt')\n",
    "                    tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                    pred = model(tokenized_x)\n",
    "                    y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                    y_true.append(1)\n",
    "                    keys = list(tokenized_x.keys())\n",
    "                    for k in keys:\n",
    "                        del tokenized_x[k]\n",
    "                    del tokenized_x\n",
    "                else:\n",
    "                    y_pred.append(1)\n",
    "                    y_true.append(1)\n",
    "    print(f\"\"\"accuracy: {accuracy_score(y_true, y_pred):.2f}\n",
    "    f1: {f1_score(y_true, y_pred):.2f}\n",
    "    precision: {precision_score(y_true, y_pred):.2f}\n",
    "    recall: {recall_score(y_true, y_pred):.2f}\"\"\")\n",
    "full_test(best_model, validation_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Argument Relation Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_premises(idx_1, argument):\n",
    "    idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    while idx_1 == idx_2:\n",
    "       idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    premise_1 = argument[\"premises\"][idx_1]\n",
    "    premise_2 = argument[\"premises\"][idx_2]\n",
    "    return {\n",
    "        \"e1\": clauses[premise_1],\n",
    "        \"e2\": clauses[premise_2]\n",
    "    }, (premise_1, premise_2)\n",
    "\n",
    "def add_premise_conclusion(idx_1, argument):\n",
    "    premise = argument[\"premises\"][idx_1]\n",
    "    conclusion = argument[\"conclusion\"]\n",
    "    if randint(1, 10) > 5: # with 50% chance we make the premise the first element\n",
    "        return {\n",
    "            \"e1\": clauses[premise],\n",
    "            \"e2\": clauses[conclusion]\n",
    "        }, (premise, conclusion)\n",
    "    else: # with 50% chance we make the conclusion the first element\n",
    "        return {\n",
    "            \"e2\": clauses[conclusion],\n",
    "            \"e1\": clauses[premise]\n",
    "        }, (premise, conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 7434 7434\n",
      "10430\n",
      "5161 5269\n"
     ]
    }
   ],
   "source": [
    "args_set = set()    #Prepare ids of clauses that are parts of arguments\n",
    "for case in refactored_dataset: # for each case\n",
    "    for argument in case[\"arguments\"]: \n",
    "        args_set.add(argument[\"conclusion\"])\n",
    "        for premise in argument[\"premises\"]:\n",
    "            args_set.add(premise)\n",
    "\n",
    "ARM_x = []\n",
    "ARM_y = []\n",
    "for caseidx in range(len(dataset)):\n",
    "    sorted_clauses = sorted(dataset[caseidx]['clauses'], key = lambda x: x['start'])\n",
    "    for i in range(len(sorted_clauses)-1):\n",
    "        if sorted_clauses[i]['_id'] in args_set:\n",
    "            for el in sorted_clauses[i+1:i+6]:\n",
    "                if el['_id'] in args_set:\n",
    "                    ARM_x.append({'e1': refactored_dataset[caseidx]['all_clauses'][sorted_clauses[i]['_id']], 'e2': refactored_dataset[caseidx]['all_clauses'][el['_id']]})\n",
    "                    y = torch.tensor([0., 1.])\n",
    "                    for arg in refactored_dataset[caseidx]['arguments']:\n",
    "                        if sorted_clauses[i]['_id'] in arg['premises'] or sorted_clauses[i]['_id'] == arg['conclusion']:\n",
    "                            if el['_id'] in arg['premises'] or sorted_clauses[i]['_id'] == arg['conclusion']:\n",
    "                                y = torch.tensor([1., 0.])\n",
    "                    ARM_y.append(y)\n",
    "    if caseidx == 33:\n",
    "        print(caseidx, len(ARM_x), len(ARM_y))\n",
    "\n",
    "print(len(ARM_x))\n",
    "print(len([ARM_y[i] for i in range(len(ARM_y)) if ARM_y[i][0]]), len([ARM_y[i] for i in range(len(ARM_y)) if not ARM_y[i][0]]))\n",
    "\n",
    "ARM_x_train = ARM_x[:6313] #Visto a mano, fino a indice 6312 sono nel train, fino a 7433 nel val, e il resto nel test\n",
    "ARM_y_train = ARM_y[:6313]\n",
    "ARM_x_val = ARM_x[6313:7434]\n",
    "ARM_y_val = ARM_y[6313:7434]\n",
    "ARM_x_test=ARM_x[7434:]\n",
    "ARM_y_test=ARM_y[7434:]\n",
    "\n",
    "ARM_x_train_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_train], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_train[0].keys()})\n",
    "\n",
    "ARM_x_val_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_val], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_val[0].keys()})\n",
    "\n",
    "ARM_x_test_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_test], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_test[0].keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1 prende le due componenti e le analizza indipendentemente per poi dare un output\n",
    "\n",
    "Molel2 concatena componente 1 e componente 2 per analizzarle assieme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")#\"FacebookAI/roberta-base\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size * 2, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        _, encoded_input_1 = self.encoder(**input[\"e1\"], return_dict = False)\n",
    "        _, encoded_input_2 = self.encoder(**input[\"e2\"], return_dict = False)\n",
    "        encoded_input = self.dropout(torch.cat((encoded_input_1, encoded_input_2), dim=1))\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "    \n",
    "class Model2(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = {key: torch.cat((input[\"e1\"][key], input[\"e2\"][key]), dim=1) for key in input[\"e1\"].keys()}\n",
    "        _, encoded_input = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "\n",
    "model1 = Model1(2)\n",
    "model2 = Model2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARM_x_tokenized = dict_lists_to_list_of_dicts({\n",
    "#    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x], padding=True, truncation=True, return_tensors='pt'))\n",
    "#for key in ARM_x[0].keys()})\n",
    "#train_dataloader, validation_dataloader, test_dataloader = get_dataloader(ARM_x_tokenized, ARM_y, 8, [9])\n",
    "batch_size = 8\n",
    "train_dataloader, validation_dataloader, test_dataloader = (DataLoader(Dataset(ARM_x_train_tokenized, ARM_y_train), batch_size = batch_size, shuffle = True),\n",
    "                                                            DataLoader(Dataset(ARM_x_val_tokenized, ARM_y_val),batch_size = batch_size, shuffle = False), \n",
    "                                                            DataLoader(Dataset(ARM_x_test_tokenized, ARM_y_test),batch_size = batch_size, shuffle = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.442 - validation loss:      0.534                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.810 - validation accuracy:      0.726\n",
      "EPOCH 1 training f1_score:      0.786 - validation f1_score:      0.634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.408 - validation loss:      0.546                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.825 - validation accuracy:      0.723\n",
      "EPOCH 2 training f1_score:      0.803 - validation f1_score:      0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.376 - validation loss:      0.557                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.844 - validation accuracy:      0.725\n",
      "EPOCH 3 training f1_score:      0.826 - validation f1_score:      0.634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.350 - validation loss:      0.561                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.861 - validation accuracy:      0.716\n",
      "EPOCH 4 training f1_score:      0.840 - validation f1_score:      0.619\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.325 - validation loss:      0.587                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.869 - validation accuracy:      0.713\n",
      "EPOCH 5 training f1_score:      0.853 - validation f1_score:      0.611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model1.train_network(train_dataloader, \n",
    "                    validation_dataloader, \n",
    "                    torch.optim.Adam, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-6,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.565 - validation loss:      0.659                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.730 - validation accuracy:      0.640\n",
      "EPOCH 1 training f1_score:      0.702 - validation f1_score:      0.527\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.526 - validation loss:      0.624                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.762 - validation accuracy:      0.670\n",
      "EPOCH 2 training f1_score:      0.732 - validation f1_score:      0.559\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.490 - validation loss:      0.622                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.784 - validation accuracy:      0.672\n",
      "EPOCH 3 training f1_score:      0.757 - validation f1_score:      0.566\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.459 - validation loss:      0.596                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.799 - validation accuracy:      0.701\n",
      "EPOCH 4 training f1_score:      0.771 - validation f1_score:      0.600\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.423 - validation loss:      0.611                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.818 - validation accuracy:      0.690\n",
      "EPOCH 5 training f1_score:      0.795 - validation f1_score:      0.584\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model2.train_network(train_dataloader, #rispetto a model1 sembra avere risultati simili, ma ci mette più tempo\n",
    "                    validation_dataloader, \n",
    "                    torch.optim.Adam, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-6,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Premise/Conclusion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are:\n",
      "      - 662 conclusions\n",
      "      - 1857 premises\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCR_x = []\n",
    "PCR_y = []\n",
    "all_premises = set()\n",
    "all_conclusions = set()\n",
    "all_clause = set()\n",
    "for case in refactored_dataset:\n",
    "    n_clauses = case[\"n_clauses\"]\n",
    "    clauses = case[\"all_clauses\"]\n",
    "    for argument in case[\"arguments\"]:\n",
    "        all_conclusions.add(clauses[argument[\"conclusion\"]])\n",
    "        all_clause.add(clauses[argument[\"conclusion\"]])\n",
    "        for premise in argument[\"premises\"]:\n",
    "            all_premises.add(clauses[premise])\n",
    "            all_clause.add(clauses[premise])\n",
    "\n",
    "for clause in all_clause:\n",
    "    current_y = torch.zeros(2)\n",
    "    if clause in all_premises:\n",
    "        current_y[0] = 1\n",
    "    if clause in all_conclusions:\n",
    "        current_y[1] = 1\n",
    "    PCR_x.append(clause)\n",
    "    PCR_y.append(current_y)\n",
    "print(f\"\"\"\n",
    "There are:\n",
    "      - {len([y for y in PCR_y if y[1] == 1])} conclusions\n",
    "      - {len([y for y in PCR_y if y[0] == 1])} premises\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
