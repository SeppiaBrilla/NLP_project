{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import numpy as np\n",
    "from neuralNetwork import NeuralNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import shuffle, randint\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from helper import dict_lists_to_list_of_dicts, get_dataloader\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ECHR_Corpus.json\")\n",
    "dataset = load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "ogni caso nel dataset è organizzato così:\n",
    "```json\n",
    "{\n",
    "    \"text\":\"il testo completo della sentenza\"\n",
    "    \"clauses\":[\n",
    "        {\n",
    "            \"_id\": \"id della clause\",\n",
    "            \"start\": \"index di inizio della clause\",\n",
    "            \"end\": \"index di fine della cluause\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "    \"argument\":[\n",
    "        {\n",
    "            \"premises\":[\n",
    "                \"id premise1\", \"id premise2\", \"...\"\n",
    "            ],\n",
    "            \"conclusion\": \"id conclusion\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "In questo passaggio riorganizzo i dati per avere comunque la stessa struttura ma trasformo le clauses in un dizionario che ha come id la chiave della clause e come valore il testo (senza quindi dover usare start e end per cercarlo nel testo). Non tutte le clause sono parte di una premessa o di una conclusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_dataset = []\n",
    "for datapoint in dataset:\n",
    "    text = datapoint[\"text\"]\n",
    "    dict_clauses = {}\n",
    "    for clause in datapoint[\"clauses\"]:\n",
    "        start = clause[\"start\"]\n",
    "        end = clause[\"end\"]\n",
    "        id = clause[\"_id\"]\n",
    "        dict_clauses[id] = text[start:end]\n",
    "    refactored_dataset.append({\n",
    "        \"text\": text,\n",
    "        \"arguments\": datapoint[\"arguments\"],\n",
    "        \"n_clauses\": len(datapoint[\"clauses\"]),\n",
    "        \"all_clauses\": dict_clauses\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondo me ha poco senso salvare il dataset come dataframe dato che è praticamente solo testo. Comunque non dovrebbe essere difficilissimo tirarci fuori qualche statistica. Ho fatto degli esempi scemi qua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On average, a case has 248.95 clauses with a median of 226 clauses per case.\n",
      "On average, a case has 17.69 arguments with a median of 14 arguments per case.\n",
      "Each argument, on average, has: 2.63 premises with a median of 2 premises per argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_arguments = []\n",
    "n_premises = []\n",
    "n_clauses = []\n",
    "for case in refactored_dataset:\n",
    "    n_arguments.append(len(case[\"arguments\"]))\n",
    "    n_clauses.append(case[\"n_clauses\"])\n",
    "    for argument in case[\"arguments\"]:\n",
    "        n_premises.append(len(argument[\"premises\"]))\n",
    "print(f\"\"\"\n",
    "On average, a case has {np.mean(n_clauses):.2f} clauses with a median of {np.median(n_clauses):.0f} clauses per case.\n",
    "On average, a case has {np.mean(n_arguments):.2f} arguments with a median of {np.median(n_arguments):.0f} arguments per case.\n",
    "Each argument, on average, has: {np.mean(n_premises):.2f} premises with a median of {np.median(n_premises):.0f} premises per argument.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qua puoi vedere come tirare fuori il testo di una conclusion o di una premise: ti basta usare la conclusion/premise come indice nel dizionario delle clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here an example of an argument:\n",
      "    - Premises:\n",
      "        The Commission notes that the applicant was detained after having been sentenced by the first instance court to 18 months' imprisonment.\n",
      "\tHe was released after the Court of Appeal reviewed this sentence, reducing it to 15 months' imprisonment, convertible to a fine.\n",
      "    - Conclusion:\n",
      "        The Commission finds that the applicant was deprived of his liberty \"after conviction by a competent court\" within the meaning of Article 5 para. 1 (a) (Art. 5-1-a) of the Convention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premise = \"\\n\\t\".join([refactored_dataset[0][\"all_clauses\"][premise] for premise in refactored_dataset[0][\"arguments\"][0][\"premises\"]])\n",
    "conclusion = refactored_dataset[0][\"all_clauses\"][refactored_dataset[0][\"arguments\"][0][\"conclusion\"]]\n",
    "print(f\"\"\"\n",
    "Here an example of an argument:\n",
    "    - Premises:\n",
    "        {premise}\n",
    "    - Conclusion:\n",
    "        {conclusion}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Argument Clause Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dato che non tutte le clause sono parte di una premise/conclusion, le dividiamo per cercare di tirare fuori un classificatore di clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are:\n",
      "      - 10456 clause\n",
      "      - 2694 true clause\n",
      "      - 7762 fake clause\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ACR_x = []\n",
    "ACR_y = []\n",
    "for case in refactored_dataset:\n",
    "    n_clauses = case[\"n_clauses\"]\n",
    "    clauses = case[\"all_clauses\"]\n",
    "    args_set = set()\n",
    "    for argument in case[\"arguments\"]:\n",
    "        ACR_x.append(clauses[argument[\"conclusion\"]])\n",
    "        ACR_y.append(torch.tensor([1,0]))\n",
    "        args_set.add(argument[\"conclusion\"])\n",
    "        for premise in argument[\"premises\"]:\n",
    "            ACR_x.append(clauses[premise])\n",
    "            ACR_y.append(torch.tensor([1.,0]))\n",
    "            args_set.add(premise)\n",
    "    for clause_id in clauses.keys():\n",
    "        if not clause_id in args_set:\n",
    "            ACR_x.append(clauses[clause_id])\n",
    "            ACR_y.append(torch.tensor([0,1.]))\n",
    "print(f\"\"\"\n",
    "There are:\n",
    "      - {len(ACR_x)} clause\n",
    "      - {len([a for a in ACR_y if a[0] == 1])} true clause\n",
    "      - {len([a for a in ACR_y if a[0] == 0])} fake clause\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Da qui in avanti ti dovrebbe essere tutto abbastanza familiare dato che è praticamente lo stesso codice dell'assignment 2. Qua ho fatto solo 5 epoche e con un learning rate a caso ma è giusto per far vedere come funziona. Da ora si può iniziare a giocare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class Model(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        _, encoded_input = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "    \n",
    "model = Model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACR_x_tokenized = dict_lists_to_list_of_dicts(tokenizer(ACR_x, padding=True, truncation=True, return_tensors='pt'))\n",
    "train_dataloader, validation_dataloader, test_dataloader = get_dataloader(ACR_x_tokenized, ACR_y, 8, [9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operating on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"operating on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.576 - validation loss:      0.629                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.747 - validation accuracy:      0.684\n",
      "EPOCH 1 training f1_score:      0.478 - validation f1_score:      0.680\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.568 - validation loss:      0.631                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.746 - validation accuracy:      0.684\n",
      "EPOCH 2 training f1_score:      0.477 - validation f1_score:      0.680\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.567 - validation loss:      0.633                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.747 - validation accuracy:      0.684\n",
      "EPOCH 3 training f1_score:      0.471 - validation f1_score:      0.680\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.566 - validation loss:      0.634                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.747 - validation accuracy:      0.684\n",
      "EPOCH 4 training f1_score:      0.472 - validation f1_score:      0.680\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.565 - validation loss:      0.634                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.747 - validation accuracy:      0.684\n",
      "EPOCH 5 training f1_score:      0.475 - validation f1_score:      0.680\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model.train_network(train_dataloader, \n",
    "                    validation_dataloader, \n",
    "                    torch.optim.SGD, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-5,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Argument Relation Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_premises(idx_1, argument):\n",
    "    idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    while idx_1 == idx_2:\n",
    "       idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    premise_1 = argument[\"premises\"][idx_1]\n",
    "    premise_2 = argument[\"premises\"][idx_2]\n",
    "    return {\n",
    "        \"e1\": clauses[premise_1],\n",
    "        \"e2\": clauses[premise_2]\n",
    "    }, (premise_1, premise_2)\n",
    "\n",
    "def add_premise_conclusion(idx_1, argument):\n",
    "    premise = argument[\"premises\"][idx_1]\n",
    "    conclusion = argument[\"conclusion\"]\n",
    "    if randint(0, 10) > 5: # with 50% chance we make the premise the first element\n",
    "        return {\n",
    "            \"e1\": clauses[premise],\n",
    "            \"e2\": clauses[conclusion]\n",
    "        }, (premise, conclusion)\n",
    "    else: # with 50% chance we make the conclusion the first element\n",
    "        return {\n",
    "            \"e2\": clauses[conclusion],\n",
    "            \"e1\": clauses[premise]\n",
    "        }, (premise, conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are:\n",
      "      - 3194 couples\n",
      "      - 1597 related clauses:\n",
      "        - 763 are couple of clauses both premises on the same argument\n",
      "        - 834 are couple of clauses premise and conclusion on the same argument\n",
      "      - 1597 unrelated clauses:\n",
      "        - 518 are couple with two premises from different arguments\n",
      "        - 468 are couple with a premise and a conclusion from two different arguments\n",
      "        - 611 are couple with two conclusions from different arguments\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ARM_x = []\n",
    "ARM_y = []\n",
    "FACTOR = .6\n",
    "n_related_two_premises = 0\n",
    "n_related_premise_conclusion = 0\n",
    "n_unrelated_two_premises = 0\n",
    "n_unrelated_two_conclusions = 0\n",
    "n_unrelated_premise_conclusion = 0\n",
    "for case in refactored_dataset: # for each case\n",
    "    n_clauses = case[\"n_clauses\"]\n",
    "    clauses = case[\"all_clauses\"]\n",
    "\n",
    "    args_set = set() # get the number of clauses in arguments\n",
    "    for argument in case[\"arguments\"]: \n",
    "        args_set.add(argument[\"conclusion\"])\n",
    "        for premise in argument[\"premises\"]:\n",
    "            args_set.add(premise)\n",
    "\n",
    "    n_related = int(len(args_set) * FACTOR) # get the number of elements we want from a given case \n",
    "    n_unrelated = int(len(args_set) * FACTOR)\n",
    "    already_taken_couples = set()\n",
    "    related = []\n",
    "    unrelated = []\n",
    "    while len(related) < n_related: # generate untill we need more related elements \n",
    "        while True:\n",
    "            idx_argument = randint(0, len(case[\"arguments\"]) - 1) # take a random argument\n",
    "            argument = case[\"arguments\"][idx_argument]\n",
    "            idx_1 = randint(0, len(argument[\"premises\"]) - 1) #take a random premise (we know we need at least 1 premise since there is only 1 conclusion)\n",
    "            if len(argument[\"premises\"]) > 1: # some arguments have only 1 premise\n",
    "                two_premises = randint(0,10)\n",
    "                if two_premises >= 4: # with 60% chance we get 2 random premises\n",
    "                    element, (id_1, id_2) = add_two_premises(idx_1, argument)\n",
    "                    if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples): #we add them only if we do not already have selected the combination\n",
    "                        related.append(element)\n",
    "                        n_related_two_premises += 1\n",
    "                        already_taken_couples.add(id_1 + id_2)\n",
    "                        break\n",
    "                else: #with 40% chance we get a premise and a conclusion\n",
    "                    element, (id_1, id_2) = add_premise_conclusion(idx_1, argument)\n",
    "                    if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples): #only if the couple has not been already selected\n",
    "                        related.append(element)\n",
    "                        n_related_premise_conclusion += 1\n",
    "                        already_taken_couples.add(id_1 + id_2)\n",
    "                        break\n",
    "            else: # if we have only one premise we just get the premise and conclusion\n",
    "                element, (id_1, id_2) = add_premise_conclusion(idx_1, argument)\n",
    "                if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples):\n",
    "                    related.append(element)\n",
    "                    n_related_premise_conclusion += 1\n",
    "                    already_taken_couples.add(id_1 + id_2)\n",
    "                    break\n",
    "\n",
    "    while len(unrelated) < n_unrelated: # generate untill we need more unrelated elements \n",
    "        while True:\n",
    "            idx_argument_1 = randint(0, len(case[\"arguments\"]) - 1) # select 2 random arguments \n",
    "            idx_argument_2 = randint(0, len(case[\"arguments\"]) - 1)\n",
    "            while idx_argument_1 == idx_argument_2: # they must be different\n",
    "                idx_argument_2 = randint(0, len(case[\"arguments\"]) - 1)\n",
    "            argument_1 = case[\"arguments\"][idx_argument_1]\n",
    "            argument_2 = case[\"arguments\"][idx_argument_2]\n",
    "            type_relation = randint(0,9)\n",
    "            if type_relation < 3: # with 30% chance we select two premises\n",
    "                id_1 = randint(0, len(argument_1[\"premises\"]) - 1)\n",
    "                id_2 = randint(0, len(argument_2[\"premises\"]) - 1)\n",
    "                id_1 = argument_1[\"premises\"][id_1]\n",
    "                id_2 = argument_2[\"premises\"][id_2]\n",
    "                if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples): # only if the couple is not already present in the dataset\n",
    "                    unrelated.append({\n",
    "                        \"e2\": clauses[id_1],\n",
    "                        \"e1\": clauses[id_2]\n",
    "                    })\n",
    "                    n_unrelated_two_premises += 1\n",
    "                    already_taken_couples.add(id_1 + id_2)\n",
    "                    break\n",
    "            elif type_relation < 6: # with 30% chance we select a premise and a conclusion\n",
    "                prem = randint(0, 1) # we select the premise argument at random\n",
    "                args = [argument_1, argument_2]\n",
    "                id_1 = randint(0, len(args[prem][\"premises\"]) - 1)\n",
    "                id_1 = args[prem][\"premises\"][id_1]\n",
    "                id_2 = args[abs(prem - 1)][\"conclusion\"] # the other argument gives the conclusion\n",
    "                if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples): # only if the couple is not already present in the dataset\n",
    "                    n_unrelated_premise_conclusion += 1\n",
    "                    already_taken_couples.add(id_1 + id_2)\n",
    "                    if randint(0, 5) > 5: # with 50% chance we put the premise first \n",
    "                        unrelated.append({\n",
    "                            \"e2\": clauses[id_1],\n",
    "                            \"e1\": clauses[id_2]\n",
    "                        })\n",
    "                    else: # with 50% chance we put the conclusion first \n",
    "                        unrelated.append({\n",
    "                            \"e2\": clauses[id_2],\n",
    "                            \"e1\": clauses[id_1]\n",
    "                        })\n",
    "                    break\n",
    "            else: # with 30% chance we select 2 conclusions\n",
    "                id_1 = argument_1[\"conclusion\"]\n",
    "                id_2 = argument_2[\"conclusion\"]\n",
    "                if not (id_1 + id_2 in already_taken_couples or id_2 + id_1 in already_taken_couples): # only if the couple is not already present in the dataset\n",
    "                    unrelated.append({\n",
    "                        \"e2\": clauses[id_1],\n",
    "                        \"e1\": clauses[id_2]\n",
    "                    })\n",
    "                    n_unrelated_two_conclusions += 1\n",
    "                    already_taken_couples.add(id_1 + id_2)\n",
    "                    break\n",
    "\n",
    "    for related_elem in related: # add all the new elements to the dataset\n",
    "        ARM_x.append(related_elem)\n",
    "        ARM_y.append(torch.tensor([1., 0]))\n",
    "    for unrelated_elem in unrelated:\n",
    "        ARM_x.append(unrelated_elem)\n",
    "        ARM_y.append(torch.tensor([0, 1.]))\n",
    "        \n",
    "assert len([y for y in ARM_y if y[0] == 0]) == (n_unrelated_two_premises + n_unrelated_premise_conclusion + n_unrelated_two_conclusions)\n",
    "assert len([y for y in ARM_y if y[0] == 1.]) == (n_related_premise_conclusion + n_related_two_premises) \n",
    "print(f\"\"\"\n",
    "There are:\n",
    "      - {len(ARM_x)} couples\n",
    "      - {n_related_two_premises + n_related_premise_conclusion} related clauses:\n",
    "        - {n_related_two_premises} are couple of clauses both premises on the same argument\n",
    "        - {n_related_premise_conclusion} are couple of clauses premise and conclusion on the same argument\n",
    "      - {n_unrelated_two_premises + n_unrelated_premise_conclusion + n_unrelated_two_conclusions} unrelated clauses:\n",
    "        - {n_unrelated_two_premises} are couple with two premises from different arguments\n",
    "        - {n_unrelated_premise_conclusion} are couple with a premise and a conclusion from two different arguments\n",
    "        - {n_unrelated_two_conclusions} are couple with two conclusions from different arguments\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1 prende le due componenti e le analizza indipendentemente per poi dare un output\n",
    "\n",
    "Molel2 concatena componente 1 e componente 2 per analizzarle assieme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class Model1(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size * 2, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        _, encoded_input_1 = self.encoder(**input[\"e1\"], return_dict = False)\n",
    "        _, encoded_input_2 = self.encoder(**input[\"e2\"], return_dict = False)\n",
    "        encoded_input = self.dropout(torch.cat((encoded_input_1, encoded_input_2), dim=1))\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "    \n",
    "class Model2(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = {key: torch.cat((input[\"e1\"][key], input[\"e2\"][key]), dim=1) for key in input[\"e1\"].keys()}\n",
    "        _, encoded_input = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "\n",
    "model1 = Model1(2)\n",
    "model2 = Model2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARM_x_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x[0].keys()})\n",
    "train_dataloader, validation_dataloader, test_dataloader = get_dataloader(ARM_x_tokenized, ARM_y, 8, [9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.696 - validation loss:      0.711                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.500 - validation accuracy:      0.527\n",
      "EPOCH 1 training f1_score:      0.328 - validation f1_score:      0.513\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.696 - validation loss:      0.712                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.501 - validation accuracy:      0.527\n",
      "EPOCH 2 training f1_score:      0.324 - validation f1_score:      0.513\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.696 - validation loss:      0.712                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.501 - validation accuracy:      0.530\n",
      "EPOCH 3 training f1_score:      0.330 - validation f1_score:      0.516\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.695 - validation loss:      0.712                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.499 - validation accuracy:      0.534\n",
      "EPOCH 4 training f1_score:      0.337 - validation f1_score:      0.463\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.695 - validation loss:      0.712                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.499 - validation accuracy:      0.534\n",
      "EPOCH 5 training f1_score:      0.333 - validation f1_score:      0.487\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model1.train_network(train_dataloader, \n",
    "                    validation_dataloader, \n",
    "                    torch.optim.SGD, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-5,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.699 - validation loss:      0.721                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.500 - validation accuracy:      0.473\n",
      "EPOCH 1 training f1_score:      0.323 - validation f1_score:      0.445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.697 - validation loss:      0.717                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.500 - validation accuracy:      0.473\n",
      "EPOCH 2 training f1_score:      0.324 - validation f1_score:      0.445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.696 - validation loss:      0.715                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.499 - validation accuracy:      0.473\n",
      "EPOCH 3 training f1_score:      0.325 - validation f1_score:      0.445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.695 - validation loss:      0.713                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.499 - validation accuracy:      0.473\n",
      "EPOCH 4 training f1_score:      0.329 - validation f1_score:      0.445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.695 - validation loss:      0.713                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.499 - validation accuracy:      0.473\n",
      "EPOCH 5 training f1_score:      0.326 - validation f1_score:      0.445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model2.train_network(train_dataloader, \n",
    "                    validation_dataloader, \n",
    "                    torch.optim.SGD, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-5,\n",
    "                    epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
