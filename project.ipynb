{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "import numpy as np\n",
    "from neuralNetwork import NeuralNetwork, In_between_epochs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import shuffle, randint\n",
    "from transformers import RobertaModel, RobertaTokenizer, AutoTokenizer, AutoModel\n",
    "from helper import dict_lists_to_list_of_dicts, get_train_valdiation_test_split, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ECHR_Corpus.json\",encoding= 'utf-8')\n",
    "dataset = load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "ogni caso nel dataset è organizzato così:\n",
    "```json\n",
    "{\n",
    "    \"text\":\"il testo completo della sentenza\"\n",
    "    \"clauses\":[\n",
    "        {\n",
    "            \"_id\": \"id della clause\",\n",
    "            \"start\": \"index di inizio della clause\",\n",
    "            \"end\": \"index di fine della cluause\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "    \"argument\":[\n",
    "        {\n",
    "            \"premises\":[\n",
    "                \"id premise1\", \"id premise2\", \"...\"\n",
    "            ],\n",
    "            \"conclusion\": \"id conclusion\"\n",
    "        },\n",
    "        {\n",
    "            \"...\":\"...\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "In questo passaggio riorganizzo i dati per avere comunque la stessa struttura ma trasformo le clauses in un dizionario che ha come id la chiave della clause e come valore il testo (senza quindi dover usare start e end per cercarlo nel testo). Non tutte le clause sono parte di una premessa o di una conclusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactored_dataset = []\n",
    "for datapoint in dataset:\n",
    "    text = datapoint[\"text\"]\n",
    "    dict_clauses = {}\n",
    "    for clause in datapoint[\"clauses\"]:\n",
    "        start = clause[\"start\"]\n",
    "        end = clause[\"end\"]\n",
    "        id = clause[\"_id\"]\n",
    "        dict_clauses[id] = text[start:end]\n",
    "    refactored_dataset.append({\n",
    "        \"text\": text,\n",
    "        \"arguments\": datapoint[\"arguments\"],\n",
    "        \"n_clauses\": len(datapoint[\"clauses\"]),\n",
    "        \"all_clauses\": dict_clauses\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondo me ha poco senso salvare il dataset come dataframe dato che è praticamente solo testo. Comunque non dovrebbe essere difficilissimo tirarci fuori qualche statistica. Ho fatto degli esempi scemi qua:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On average, a case has 248.95 clauses with a median of 226 clauses per case.\n",
      "On average, a case has 17.69 arguments with a median of 14 arguments per case.\n",
      "Each argument, on average, has: 2.63 premises with a median of 2 premises per argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_arguments = []\n",
    "n_premises = []\n",
    "n_clauses = []\n",
    "for case in refactored_dataset:\n",
    "    n_arguments.append(len(case[\"arguments\"]))\n",
    "    n_clauses.append(case[\"n_clauses\"])\n",
    "    for argument in case[\"arguments\"]:\n",
    "        n_premises.append(len(argument[\"premises\"]))\n",
    "print(f\"\"\"\n",
    "On average, a case has {np.mean(n_clauses):.2f} clauses with a median of {np.median(n_clauses):.0f} clauses per case.\n",
    "On average, a case has {np.mean(n_arguments):.2f} arguments with a median of {np.median(n_arguments):.0f} arguments per case.\n",
    "Each argument, on average, has: {np.mean(n_premises):.2f} premises with a median of {np.median(n_premises):.0f} premises per argument.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qua puoi vedere come tirare fuori il testo di una conclusion o di una premise: ti basta usare la conclusion/premise come indice nel dizionario delle clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here an example of an argument:\n",
      "    - Premises:\n",
      "        The Commission notes that the applicant was detained after having been sentenced by the first instance court to 18 months' imprisonment.\n",
      "\tHe was released after the Court of Appeal reviewed this sentence, reducing it to 15 months' imprisonment, convertible to a fine.\n",
      "    - Conclusion:\n",
      "        The Commission finds that the applicant was deprived of his liberty \"after conviction by a competent court\" within the meaning of Article 5 para. 1 (a) (Art. 5-1-a) of the Convention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "premise = \"\\n\\t\".join([refactored_dataset[0][\"all_clauses\"][premise] for premise in refactored_dataset[0][\"arguments\"][0][\"premises\"]])\n",
    "conclusion = refactored_dataset[0][\"all_clauses\"][refactored_dataset[0][\"arguments\"][0][\"conclusion\"]]\n",
    "print(f\"\"\"\n",
    "Here an example of an argument:\n",
    "    - Premises:\n",
    "        {premise}\n",
    "    - Conclusion:\n",
    "        {conclusion}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Argument Clause Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = get_train_valdiation_test_split(refactored_dataset, [4])#, seed = 42) #42, 4, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 28 cases in the training set (fold 0), \n",
      "    6 cases in the validation set (fold 0) and\n",
      "    8 cases in the test set (fold 0)\n",
      "    \n",
      "there are 28 cases in the training set (fold 1), \n",
      "    6 cases in the validation set (fold 1) and\n",
      "    8 cases in the test set (fold 1)\n",
      "    \n",
      "there are 28 cases in the training set (fold 2), \n",
      "    6 cases in the validation set (fold 2) and\n",
      "    8 cases in the test set (fold 2)\n",
      "    \n",
      "there are 28 cases in the training set (fold 3), \n",
      "    6 cases in the validation set (fold 3) and\n",
      "    8 cases in the test set (fold 3)\n",
      "    \n",
      "there are 28 cases in the training set (fold 4), \n",
      "    6 cases in the validation set (fold 4) and\n",
      "    8 cases in the test set (fold 4)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cases)):\n",
    "    train_cases, validation_cases, test_cases = cases[i]\n",
    "    print(f\"\"\"there are {len(train_cases)} cases in the training set (fold {i}), \n",
    "    {len(validation_cases)} cases in the validation set (fold {i}) and\n",
    "    {len(test_cases)} cases in the test set (fold {i})\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dato che non tutte le clause sono parte di una premise/conclusion, le dividiamo per cercare di tirare fuori un classificatore di clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "\n",
      "    There are:\n",
      "        - 3355 clause\n",
      "        - 1846 true clause\n",
      "        - 1509 fake clause\n",
      "    \n",
      "validation set:\n",
      "\n",
      "    There are:\n",
      "        - 288 clause\n",
      "        - 223 true clause\n",
      "        - 65 fake clause\n",
      "    \n",
      "test set:\n",
      "\n",
      "    There are:\n",
      "        - 1046 clause\n",
      "        - 624 true clause\n",
      "        - 422 fake clause\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def get_acr_dataloader(cases, tokenizer, batch_size=16, shuffle=False, verbose=True):\n",
    "    ACR_x = []\n",
    "    ACR_y = []\n",
    "    for case in cases:\n",
    "        n_clauses = case[\"n_clauses\"]\n",
    "        clauses = case[\"all_clauses\"]\n",
    "        args_set = set()\n",
    "        splitter = \"AS TO THE LAW\" if \"AS TO THE LAW\" in case[\"text\"] else \"THE LAW\"\n",
    "        law_section = case[\"text\"].split(splitter)[1]\n",
    "        for argument in case[\"arguments\"]:\n",
    "            if not argument[\"conclusion\"] in args_set and clauses[argument[\"conclusion\"]] in law_section:\n",
    "                ACR_x.append(clauses[argument[\"conclusion\"]])\n",
    "                ACR_y.append(torch.tensor([1.,0.]))\n",
    "                args_set.add(argument[\"conclusion\"])\n",
    "            for premise in argument[\"premises\"]:\n",
    "                if not premise in args_set and clauses[premise] in law_section:\n",
    "                    ACR_x.append(clauses[premise])\n",
    "                    ACR_y.append(torch.tensor([1.,0.]))\n",
    "                    args_set.add(premise)\n",
    "        for clause_id in clauses.keys():\n",
    "            if not clause_id in args_set and clauses[clause_id] in law_section:\n",
    "                ACR_x.append(clauses[clause_id])\n",
    "                ACR_y.append(torch.tensor([0.,1.]))\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "    There are:\n",
    "        - {len(ACR_x)} clause\n",
    "        - {len([a for a in ACR_y if a[0] == 1])} true clause\n",
    "        - {len([a for a in ACR_y if a[0] == 0])} fake clause\n",
    "    \"\"\")\n",
    "    ACR_x_tokenized = dict_lists_to_list_of_dicts(tokenizer(ACR_x, padding=True, truncation=True, return_tensors='pt'))\n",
    "    dataset = Dataset(ACR_x_tokenized, ACR_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "train_cases, validation_cases, test_cases = cases[-1]\n",
    "print(\"train set:\")\n",
    "train_dataloader = get_acr_dataloader(train_cases, tokenizer, shuffle=True, batch_size=8)\n",
    "print(\"validation set:\")\n",
    "validation_dataloder = get_acr_dataloader(validation_cases, tokenizer, batch_size=8)\n",
    "print(\"test set:\")\n",
    "test_dataloader = get_acr_dataloader(test_cases, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Da qui in avanti ti dovrebbe essere tutto abbastanza familiare dato che è praticamente lo stesso codice dell'assignment 2. Qua ho fatto solo 5 epoche e con un learning rate a caso ma è giusto per far vedere come funziona. Da ora si può iniziare a giocare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoded_input, _ = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "        encoded_input = torch.nn.functional.avg_pool1d(\n",
    "            encoded_input.permute(0, 2, 1), \n",
    "            kernel_size=encoded_input.size(1)\n",
    "        ).squeeze(2)\n",
    "        if len(encoded_input.size()) != 2:\n",
    "            print(encoded_input.size())\n",
    "        return self.output_layer(encoded_input).float()\n",
    "\n",
    "    def freeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    def unfreeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "model = Model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operating on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"operating on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(In_between_epochs):\n",
    "    def __init__(self, delta, patience):\n",
    "        self.delta = delta\n",
    "        self.patience = patience\n",
    "        self.current_patience = 0\n",
    "        self.best_valid_loss = 100000000000000000000\n",
    "        self.best_model = Model(2)\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def __call__(self, model:torch.nn.Module, loaders:dict[str,torch.utils.data.DataLoader], device:'torch.device|str', output_extraction_function, losses:dict[str, float]) -> bool:\n",
    "        self.epochs += 1\n",
    "        if losses[\"validation\"] < self.best_valid_loss - self.delta:\n",
    "            self.best_valid_loss = losses[\"validation\"]\n",
    "            self.best_model.load_state_dict(model.state_dict())\n",
    "            self.current_patience = 0\n",
    "        else:\n",
    "            self.current_patience += 1\n",
    "            if self.current_patience >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    def reset(self):\n",
    "        self.current_patience = 0\n",
    "        self.epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train, validation, min_lr, start_lr, early_stopping, frac):\n",
    "    tot_train_data, tot_val_data = [], []\n",
    "    lr = start_lr\n",
    "    while lr > min_lr:\n",
    "        train_data, validation_data = model.train_network(train, \n",
    "                        validation, \n",
    "                        torch.optim.Adam, \n",
    "                        loss_function=nn.CrossEntropyLoss(),\n",
    "                        device=device, \n",
    "                        batch_size=32,\n",
    "                        verbose=True, \n",
    "                        output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                        metrics={\n",
    "                        \"accuracy\": accuracy_score, \n",
    "                        \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                        in_between_epochs = {\"early_stopping\": early_stopping},\n",
    "                        learning_rate=lr,\n",
    "                        epochs=2)\n",
    "        train_data[\"epochs\"] = early_stopping.epochs\n",
    "        train_data[\"lr\"] = lr\n",
    "        validation_data[\"epochs\"] = early_stopping.epochs\n",
    "        validation_data[\"lr\"] = lr\n",
    "        tot_train_data.append(train_data)\n",
    "        tot_val_data.append(validation_data)\n",
    "        model.load_state_dict(early_stopping.best_model.state_dict())\n",
    "        lr = lr * frac\n",
    "        early_stopping.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_train(lrs:list[float], train, validation):\n",
    "    trainings = []\n",
    "    models = []\n",
    "    for lr in lrs:\n",
    "        model = Model(2)\n",
    "        train_data, validation_data = model.train_network(train, \n",
    "                        validation, \n",
    "                        torch.optim.Adam, \n",
    "                        loss_function=nn.CrossEntropyLoss(),\n",
    "                        device=device, \n",
    "                        batch_size=32,\n",
    "                        verbose=True, \n",
    "                        output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                        metrics={\n",
    "                        \"accuracy\": accuracy_score, \n",
    "                        \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                        learning_rate=lr,\n",
    "                        epochs=15)\n",
    "        for key in train_data:\n",
    "                train_data[key] = [float(v) for v in train_data[key]]\n",
    "                validation_data[key] = [float(v) for v in validation_data[key]]\n",
    "        trainings.append({\"train\": train_data, \"validation\":validation_data, \"lr\":lr})\n",
    "        models.append(model)\n",
    "    return trainings, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_training(cases):\n",
    "    best_models = []\n",
    "    for cases_split in cases:\n",
    "        train_cases, validation_cases, _ = cases_split\n",
    "        train_dataloader = get_acr_dataloader(train_cases, tokenizer, shuffle=True, batch_size=8, verbose=False)\n",
    "        validation_dataloder = get_acr_dataloader(validation_cases, tokenizer, batch_size=8, verbose=False)\n",
    "        model = Model(2)\n",
    "        early_stopping = EarlyStopping(.001, 3)\n",
    "        train(model, train_dataloader, validation_dataloder, 5e-7, 1e-6, early_stopping, .6)\n",
    "        model.load_state_dict(early_stopping.best_model.state_dict())\n",
    "        best_models.append({\"loss\": early_stopping.best_valid_loss, \"model\": model})\n",
    "    \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.589 - validation loss:      0.656                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.701 - validation accuracy:      0.651\n",
      "EPOCH 1 training f1_score:      0.618 - validation f1_score:      0.489\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.499 - validation loss:      0.700                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.761 - validation accuracy:      0.660\n",
      "EPOCH 2 training f1_score:      0.716 - validation f1_score:      0.499\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.533 - validation loss:      0.706                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.736 - validation accuracy:      0.605\n",
      "EPOCH 1 training f1_score:      0.691 - validation f1_score:      0.435\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.493 - validation loss:      0.690                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.766 - validation accuracy:      0.636\n",
      "EPOCH 2 training f1_score:      0.727 - validation f1_score:      0.478\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.629 - validation loss:      0.666                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.686 - validation accuracy:      0.597\n",
      "EPOCH 1 training f1_score:      0.564 - validation f1_score:      0.533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.522 - validation loss:      0.617                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.751 - validation accuracy:      0.662\n",
      "EPOCH 2 training f1_score:      0.678 - validation f1_score:      0.540\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.490 - validation loss:      0.617                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.770 - validation accuracy:      0.667\n",
      "EPOCH 1 training f1_score:      0.705 - validation f1_score:      0.543\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.463 - validation loss:      0.625                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.791 - validation accuracy:      0.669\n",
      "EPOCH 2 training f1_score:      0.734 - validation f1_score:      0.544\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.628 - validation loss:      0.644                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.680 - validation accuracy:      0.649\n",
      "EPOCH 1 training f1_score:      0.619 - validation f1_score:      0.514\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.538 - validation loss:      0.592                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.731 - validation accuracy:      0.684\n",
      "EPOCH 2 training f1_score:      0.667 - validation f1_score:      0.571\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.503 - validation loss:      0.570                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.756 - validation accuracy:      0.697\n",
      "EPOCH 1 training f1_score:      0.704 - validation f1_score:      0.583\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.476 - validation loss:      0.556                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.779 - validation accuracy:      0.696\n",
      "EPOCH 2 training f1_score:      0.735 - validation f1_score:      0.555\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.635 - validation loss:      0.614                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.645 - validation accuracy:      0.716\n",
      "EPOCH 1 training f1_score:      0.511 - validation f1_score:      0.636\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.538 - validation loss:      0.502                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.717 - validation accuracy:      0.809\n",
      "EPOCH 2 training f1_score:      0.638 - validation f1_score:      0.682\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.492 - validation loss:      0.505                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.768 - validation accuracy:      0.788\n",
      "EPOCH 1 training f1_score:      0.729 - validation f1_score:      0.614\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.476 - validation loss:      0.476                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.762 - validation accuracy:      0.817\n",
      "EPOCH 2 training f1_score:      0.708 - validation f1_score:      0.673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.649 - validation loss:      0.621                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.661 - validation accuracy:      0.781\n",
      "EPOCH 1 training f1_score:      0.574 - validation f1_score:      0.651\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.569 - validation loss:      0.504                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.707 - validation accuracy:      0.816\n",
      "EPOCH 2 training f1_score:      0.637 - validation f1_score:      0.682\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 1 training loss:      0.532 - validation loss:      0.484                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.733 - validation accuracy:      0.812\n",
      "EPOCH 1 training f1_score:      0.679 - validation f1_score:      0.653\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.510 - validation loss:      0.470                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.745 - validation accuracy:      0.799\n",
      "EPOCH 2 training f1_score:      0.695 - validation f1_score:      0.652\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = fold_training(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model loss: 0.47013330459594727\n"
     ]
    }
   ],
   "source": [
    "best_model_fold = min(models, key= lambda x: x[\"loss\"]) \n",
    "print(f\"best model loss: {best_model_fold['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_fold['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_fold['model'].state_dict(), 'best model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = Model(2)\n",
    "best_model.load_state_dict(best_model_fold[\"model\"].state_dict())\n",
    "best_model = best_model.to(device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "1904\n",
      "accuracy: 0.86\n",
      "    f1: 0.89\n",
      "    precision: 0.92\n",
      "    recall: 0.86\n"
     ]
    }
   ],
   "source": [
    "def full_test(model, cases):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for case in cases:\n",
    "            clauses = case[\"all_clauses\"]\n",
    "            args_set = set()\n",
    "            splitter = \"AS TO THE LAW\" if \"AS TO THE LAW\" in case[\"text\"] else \"THE LAW\"\n",
    "            law_section = case[\"text\"].split(splitter)[1]\n",
    "            for argument in case[\"arguments\"]:\n",
    "                if (argument[\"conclusion\"] not in args_set) and (clauses[argument[\"conclusion\"]] in law_section):\n",
    "                    tokenized_x = tokenizer(clauses[argument[\"conclusion\"]],padding = True, truncation=True, return_tensors='pt')\n",
    "                    tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                    pred = model(tokenized_x)\n",
    "                    y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                    y_true.append(0)\n",
    "                    keys = list(tokenized_x.keys())\n",
    "                    for k in keys:\n",
    "                        del tokenized_x[k]\n",
    "                    del tokenized_x\n",
    "                    args_set.add(argument[\"conclusion\"])\n",
    "                elif argument[\"conclusion\"] not in args_set and clauses[argument[\"conclusion\"]] not in law_section:\n",
    "                    args_set.add(argument[\"conclusion\"])\n",
    "                    y_pred.append(1)\n",
    "                    y_true.append(0)\n",
    "                for premise in argument[\"premises\"]:\n",
    "                    if premise not in args_set and clauses[premise] in law_section:\n",
    "                        tokenized_x = tokenizer(clauses[premise], padding = True, truncation=True, return_tensors='pt')\n",
    "                        tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                        pred = model(tokenized_x)\n",
    "                        y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                        y_true.append(0)\n",
    "                        keys = list(tokenized_x.keys())\n",
    "                        for k in keys:\n",
    "                            del tokenized_x[k]\n",
    "                        del tokenized_x\n",
    "                        args_set.add(premise)\n",
    "                        \n",
    "                    elif premise not in args_set and clauses[premise] not in law_section:\n",
    "                        args_set.add(premise)\n",
    "                        y_pred.append(1)\n",
    "                        y_true.append(0)\n",
    "            for clause_id in clauses.keys():\n",
    "                if (not clause_id in args_set) and (clauses[clause_id] in law_section):\n",
    "                    tokenized_x = tokenizer(clauses[clause_id], padding = True, truncation=True, return_tensors='pt')\n",
    "                    tokenized_x = {key: tokenized_x[key].to(device) for key in tokenized_x.keys()}\n",
    "                    pred = model(tokenized_x)\n",
    "                    y_pred.append(torch.max(pred, -1)[1].view(-1).cpu().tolist()[0])\n",
    "                    y_true.append(1)\n",
    "                    keys = list(tokenized_x.keys())\n",
    "                    for k in keys:\n",
    "                        del tokenized_x[k]\n",
    "                    del tokenized_x\n",
    "                    args_set.add(clause_id)\n",
    "                    \n",
    "                #else:\n",
    "                if (not clause_id in args_set) and (clauses[clause_id] not in law_section):\n",
    "                    y_pred.append(1)\n",
    "                    y_true.append(1)\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    print(len(y_true))\n",
    "    print(f\"\"\"accuracy: {accuracy_score(y_true, y_pred):.2f}\n",
    "    f1: {f1_score(y_true, y_pred):.2f}\n",
    "    precision: {precision_score(y_true, y_pred):.2f}\n",
    "    recall: {recall_score(y_true, y_pred):.2f}\"\"\")\n",
    "full_test(best_model, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5d4b80113e582511aa1cd850': \" The\\r\\nCommission, taking into account the applicant's position in these\\r\\nproceedings and the impact of their outcome on her own financial\\r\\nsituation,\",\n",
       " '5d4b802b3e582511aa1cd851': 'finds that the applicant is entitled to complain about the\\r\\nlength of the proceedings.\\r',\n",
       " '5d4b804a3e582511aa1cd852': 'They submit that the applicant\\r\\nfailed to complain about the length in the course of the domestic court\\r\\nproceedings and did not lodge a request under S. 91 of the Court\\r\\nOrganisation Act 1990 (Gerichtsverfassungsgesetz) that, in view of the\\r\\nalleged delay, the superior court should fix an appropriate time-limit\\r\\nfor the conduct of the court proceedings.\\r',\n",
       " '5d4b80613e582511aa1cd853': 'According to Article 26 (Art. 26), the \"Commission may only deal\\r\\nwith the matter after all domestic remedies have been exhausted,\\r\\naccording to the generally recognised rules of international law\".\\r\\n\\r',\n",
       " '5d4b80763e582511aa1cd854': 'The Government maintain that the applicant failed to exhaust, as\\r\\nrequired by Article 26 (Art. 26) of the Convention, the remedies\\r\\navailable to her under Austrian law.',\n",
       " '5d4b809e3e582511aa1cd855': ' The Commission notes that S. 91 of the Austrian Court\\r\\nOrganisation Act, which entitles the parties to court proceedings to\\r\\nlodge a request with the superior court to fix a time limit in respect\\r\\nof a delayed procedural step, entered into force on 1 January 1990 when\\r\\nthe proceedings in question were already pending for one year.',\n",
       " '5d4b80b53e582511aa1cd856': \"At that\\r\\nstage, the District Court had already been dealing with the applicant's\\r\\nrequest of December 1988 and, in its decision of 10 August 1989,\\r\\npostponed the examination thereof, and the Regional Court had decided\\r\\non the applicant's appeal on 29 November 1989.\\r\",\n",
       " '5d4b80ce3e582511aa1cd857': 'The Commission finds that it is thus not faced with the issue of\\r\\nan alleged absence of any reaction of the competent court to a\\r\\nprocedural request (cf. No. 19369/92, Dec. 8.1.93, not published).',\n",
       " '5d4b80f23e582511aa1cd858': 'The Commission finds that it is thus not faced with the issue of\\r\\nan alleged absence of any reaction of the competent court to a\\r\\nprocedural request (cf. No. 19369/92, Dec. 8.1.93, not published).',\n",
       " '5d4b81133e582511aa1cd859': 'In\\r\\nthe circumstances of the present case, a request under S. 91 of the\\r\\nCourt Organisation Act cannot be considered as an effective remedy to\\r\\nensure, regarding the proceedings as a whole, a determination of the\\r\\napplicant\\'s civil rights and obligations within a \"reasonable time\"\\r\\nwithin the meaning of Article 6 para. 1 (Art. 6-1) of the Convention.\\r',\n",
       " '5d4b81223e582511aa1cd85a': 'The application cannot, therefore, be rejected for non-compliance\\r\\nwith the condition as to the exhaustion of domestic remedies under\\r\\nArticle 26 (Art. 26) of the Convention.\\r',\n",
       " '5d4b81533e582511aa1cd85b': 'As regards the merits of the complaint, the Government, referring\\r\\nto the case-law of the Convention organs, argue that the length of the\\r\\nproceedings was mainly due to the complexity of the case.',\n",
       " '5d4b816e3e582511aa1cd85c': 'In this\\r\\nrespect, they refer to the numerous other matters related to the\\r\\nproceedings at issue, and the proceedings before the Vienna Juvenile\\r\\nCourt.',\n",
       " '5d4b81823e582511aa1cd85d': 'They consider that no substantial delays were imputable to the\\r\\nAustrian authorities.',\n",
       " '5d4b81e33e582511aa1cd85f': 'thereby caused difficulties to survey the file and to\\r\\ndecide upon all of her requests in due time.\\r',\n",
       " '5d4b822c3e582511aa1cd861': ' According to the Government, the applicant\\r\\nherself was responsible for the delays in',\n",
       " '5d4b824f3e582511aa1cd862': 'that she filed numerous\\r\\nsubmissions',\n",
       " '5d4b828a3e582511aa1cd863': 'The Commission considers, in the light of the criteria\\r\\nestablished by the case-law of the Convention institutions on the\\r\\nquestion of \"reasonable time\"',\n",
       " '5d4b829f3e582511aa1cd864': 'the complexity of the case,',\n",
       " '5d4b82c83e582511aa1cd865': \"the\\r\\napplicant's conduct and that of the competent authorities), and having\\r\\nregard to all the information in its possession\",\n",
       " '5d4b82d83e582511aa1cd866': 'that a thorough\\r\\nexamination of this complaint is required, both as to the law and as\\r\\nto the facts.\\r',\n",
       " '5d4b82f23e582511aa1cd867': 'For these reasons, the Commission unanimously\\r\\n\\r\\n      DECLARES THE APPLICATION ADMISSIBLE,\\r\\n      without prejudging the merits of the case.\\r\\n\\r',\n",
       " '5d4d251b3e582511aa1cde5b': 'The facts, as they have been submitted by the parties, may be\\r\\nsummarised as follows.\\r\\n\\r',\n",
       " '5d4d25243e582511aa1cde5c': 'The applicant, an Austrian citizen, born in 1951, is living in\\r\\nVienna. ',\n",
       " '5d4d252c3e582511aa1cde5d': 'The applicant is the mother of three children born in wedlock\\r\\nin 1973, 1974 and 1976, respectively.',\n",
       " '5d4d25313e582511aa1cde5e': 'The spouses separated in 1982.\\r',\n",
       " '5d4d25373e582511aa1cde5f': 'The custody over the children born in 1973 and 1974 was assigned to the\\r\\napplicant, the custody over the child born in 1976 to his father.\\r',\n",
       " '5d4d253b3e582511aa1cde60': ' Since April 1977 proceedings relating to custody and other\\r\\nmatters concerning the children are pending before the Floridsdorf\\r\\nDistrict Court (Bezirksgericht).\\r',\n",
       " '5d4d25483e582511aa1cde61': ' On 31 December 1988 the applicant, on behalf of her children born\\r\\nin 1973 and 1974, applied with the Floridsdorf District Court that her\\r\\ndivorced husband be ordered to increase his maintenance payments by\\r\\nAS 600 per month and per child, as from 1 January 1989, as compared\\r\\nwith the amount of AS 4550 per child fixed by the Court on\\r\\n12 August 1986.',\n",
       " '5d4d254b3e582511aa1cde62': 'Her request was received at the Court on\\r\\n3 January 1989.\\r\\n\\r',\n",
       " '5d4d25513e582511aa1cde63': 'On 16 February 1989 the applicant challenged the judicial officer\\r\\n(Rechtspfleger) at the Floridsdorf District Court dealing with her case\\r\\nfor bias, which she withdrew on 5 May 1989.\\r',\n",
       " '5d4d25613e582511aa1cde64': 'On 10 August 1989 the Floridsdorf District Court took a decision\\r\\non various earlier requests of the applicant regarding the increase of\\r\\nthe maintenance payments in respect of periods in 1987 and 1988, and\\r\\nother financial matters. ',\n",
       " '5d4d25663e582511aa1cde65': 'The amounts of increase for these periods\\r\\nvaried between AS 200 and AS 1100 per month.',\n",
       " '5d4d256c3e582511aa1cde66': \"The Court also stated\\r\\nthat the applicant's request regarding the maintenance payments as from\\r\\n1 January 1989 would be dealt with following further investigations.\\r\",\n",
       " '5d4d25733e582511aa1cde67': 'On 25 August 1989 the applicant appealed against the decision of\\r\\n10 August 1989 with the Vienna Regional Court (Landesgericht).\\r\\n\\r',\n",
       " '5d4d25773e582511aa1cde68': 'On 1 September 1989 the applicant amended her claims regarding\\r\\nthe period as from January 1989 to amounts of increase between AS 950\\r\\nand AS 2000 per month.\\r',\n",
       " '5d4d25863e582511aa1cde69': \"On 29 November 1989 the Vienna Regional Court (Landesgericht),\\r\\nupon the applicant's appeal (Rekurs), amended the District Court's\\r\\ndecision, granting a higher increase in respect of four months in 1987,\\r\\nand dismissed the remainder of the appeal.\",\n",
       " '5d4d258a3e582511aa1cde6a': 'The decision and the files\\r\\nwere received at the District Court on 28 December 1989.\\r',\n",
       " '5d4d25933e582511aa1cde6b': \" In the beginning of 1990 the files were forwarded to the Vienna\\r\\nJuvenile Court (Jugendgerichtshof) in the context of proceedings to\\r\\nwithdraw the applicant's custody in respect of her child born in 1973.\\r\",\n",
       " '5d4d259f3e582511aa1cde6c': \"The applicant's custody was withdrawn in April 1990, the proceedings\\r\\nwere disposed of by declaring the child of full age in January 1991,\\r\\ndecision which became final in October 1992.\",\n",
       " '5d4d25a73e582511aa1cde6d': \"Proceedings regarding\\r\\nfurther financial matters, in particular the applicant's obligation to\\r\\npay maintenance for the child concerned, continued to be pending before\\r\\nthe Vienna Juvenile Court.\",\n",
       " '5d4d25b13e582511aa1cde6f': 'In May 1993, the Juvenile Court returned\\r\\nthe files.\\r',\n",
       " '5d4d25ba3e582511aa1cde70': \"On 22 October 1993 the Floridsdorf District Court partly granted\\r\\nthe applicant's request of 31 December 1988 as amended in September\\r\\n1989, to the extent that it concerned the child born in 1974.\",\n",
       " '5d4d25be3e582511aa1cde71': 'The\\r\\nCourt granted an increase which varied between AS 200 and AS 1550 per\\r\\nmonth and dismissed the remainder of her claims.\\r',\n",
       " '5d4d25c63e582511aa1cde72': \" On 7 December 1993 the Vienna Regional Court rejected the\\r\\napplicant's appeal on behalf of her child, born in 1974, meanwhile an\\r\\nadult, and dismissed the appeal brought by this child on its own.\\r\\n\\r\",\n",
       " '5d4d25cc3e582511aa1cde73': ' The applicant complains that her request relating to a rise of\\r\\nmaintenance payments for her children as from 1 January 1989 was not\\r\\ndealt with within a reasonable time.\\r',\n",
       " '5d4d25d03e582511aa1cde74': 'The application was introduced on 10 April 1993 and registered\\r\\non 7 June 1993.\\r\\n\\r',\n",
       " '5d4d25d43e582511aa1cde75': 'On 29 June 1994 the Commission decided to communicate the\\r\\napplication to the respondent Government for observations on the\\r\\nadmissibility and merits.\\r',\n",
       " '5d4d25df3e582511aa1cde76': 'On 31 October 1994, after an extension of the time-limit, the\\r\\nGovernment submitted their observations.',\n",
       " '5d4d25e73e582511aa1cde77': 'The observations in reply by\\r\\nthe applicant were submitted on 8 December 1994.\\r',\n",
       " '5d53a9c43e582511aa1ce5d7': 'The applicant complains about the length of the proceedings\\r\\nregarding her request of 31 December 1988 for an increase of\\r\\nmaintenance payments in respect of two of her children.\\r',\n",
       " '5d53a9ca3e582511aa1ce5d8': 'Article 6 para. 1 (Art. 6-1), so far as relevant, provides that\\r\\n\"in the determination of his civil rights and obligations ..., everyone\\r\\nis entitled to a ... hearing within a reasonable time\".\\r',\n",
       " '5d53a9d33e582511aa1ce5d9': 'The proceedings at issue concerned maintenance claims and fall\\r\\nto be examined under Article 6 para. 1 (Art. 6-1) of the Convention.\\r',\n",
       " '5d53a9ec3e582511aa1ce5da': 'The Commission notes that the applicant brought the proceedings on\\r\\nbehalf of her children, who were minor at the relevant time.',\n",
       " '5d53fbb23e582511aa1ce79e': 'The European Commission of Human Rights (First Chamber) sitting\\r\\nin private on 17 May 1995, the following members being present:',\n",
       " '5f94595dbf181507836fa365': 'Having regard to Article 25 of the Convention for the Protection\\r\\nof Human Rights and Fundamental Freedoms;',\n",
       " '5f945962bf181507836fa366': 'Having regard to the application introduced on 10 April 1993 by\\r\\nElisabeth Girardi against Austria and registered on 7 June 1993 under\\r\\nfile No. 21985/93;',\n",
       " '5f945966bf181507836fa367': 'Having regard to the report provided for in Rule 47 of the Rules\\r\\nof Procedure of the Commission;',\n",
       " '5f94596cbf181507836fa368': 'Having regard to the observations submitted by the respondent\\r\\nGovernment on 31 October 1994, after an extension of the time-limit and\\r\\nthe observations in reply submitted by the applicant on\\r\\n8 December 1994;',\n",
       " '5f945970bf181507836fa369': 'Having deliberated;',\n",
       " '5f945973bf181507836fa36a': 'Decides as follows:'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clauses = test_cases[0][\"all_clauses\"]\n",
    "clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_x = tokenizer(test_cases[0][\"all_clauses\"][list(test_cases[0][\"all_clauses\"].keys())[2]], truncation=True, return_tensors='pt')\n",
    "model(tokenized_x)\n",
    "\n",
    "torch.max(model(tokenized_x), -1)[1].view(-1).cpu().tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Argument Relation Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_two_premises(idx_1, argument):\n",
    "    idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    while idx_1 == idx_2:\n",
    "       idx_2 = randint(0, len(argument[\"premises\"]) - 1)\n",
    "    premise_1 = argument[\"premises\"][idx_1]\n",
    "    premise_2 = argument[\"premises\"][idx_2]\n",
    "    return {\n",
    "        \"e1\": clauses[premise_1],\n",
    "        \"e2\": clauses[premise_2]\n",
    "    }, (premise_1, premise_2)\n",
    "\n",
    "def add_premise_conclusion(idx_1, argument):\n",
    "    premise = argument[\"premises\"][idx_1]\n",
    "    conclusion = argument[\"conclusion\"]\n",
    "    if randint(1, 10) > 5: # with 50% chance we make the premise the first element\n",
    "        return {\n",
    "            \"e1\": clauses[premise],\n",
    "            \"e2\": clauses[conclusion]\n",
    "        }, (premise, conclusion)\n",
    "    else: # with 50% chance we make the conclusion the first element\n",
    "        return {\n",
    "            \"e2\": clauses[conclusion],\n",
    "            \"e1\": clauses[premise]\n",
    "        }, (premise, conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 7434 7434\n",
      "10430\n",
      "5161 5269\n"
     ]
    }
   ],
   "source": [
    "args_set = set()    #Prepare ids of clauses that are parts of arguments\n",
    "for case in refactored_dataset: # for each case\n",
    "    for argument in case[\"arguments\"]: \n",
    "        args_set.add(argument[\"conclusion\"])\n",
    "        for premise in argument[\"premises\"]:\n",
    "            args_set.add(premise)\n",
    "\n",
    "ARM_x = []\n",
    "ARM_y = []\n",
    "for caseidx in range(len(dataset)):\n",
    "    sorted_clauses = sorted(dataset[caseidx]['clauses'], key = lambda x: x['start'])\n",
    "    for i in range(len(sorted_clauses)-1):\n",
    "        if sorted_clauses[i]['_id'] in args_set:\n",
    "            for el in sorted_clauses[i+1:i+6]:\n",
    "                if el['_id'] in args_set:\n",
    "                    ARM_x.append({'e1': refactored_dataset[caseidx]['all_clauses'][sorted_clauses[i]['_id']], 'e2': refactored_dataset[caseidx]['all_clauses'][el['_id']]})\n",
    "                    y = torch.tensor([0., 1.])\n",
    "                    for arg in refactored_dataset[caseidx]['arguments']:\n",
    "                        if sorted_clauses[i]['_id'] in arg['premises'] or sorted_clauses[i]['_id'] == arg['conclusion']:\n",
    "                            if el['_id'] in arg['premises'] or sorted_clauses[i]['_id'] == arg['conclusion']:\n",
    "                                y = torch.tensor([1., 0.])\n",
    "                    ARM_y.append(y)\n",
    "    if caseidx == 33:\n",
    "        print(caseidx, len(ARM_x), len(ARM_y))\n",
    "\n",
    "print(len(ARM_x))\n",
    "print(len([ARM_y[i] for i in range(len(ARM_y)) if ARM_y[i][0]]), len([ARM_y[i] for i in range(len(ARM_y)) if not ARM_y[i][0]]))\n",
    "\n",
    "ARM_x_train = ARM_x[:6313] #Visto a mano, fino a indice 6312 sono nel train, fino a 7433 nel val, e il resto nel test\n",
    "ARM_y_train = ARM_y[:6313]\n",
    "ARM_x_val = ARM_x[6313:7434]\n",
    "ARM_y_val = ARM_y[6313:7434]\n",
    "ARM_x_test=ARM_x[7434:]\n",
    "ARM_y_test=ARM_y[7434:]\n",
    "\n",
    "ARM_x_train_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_train], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_train[0].keys()})\n",
    "\n",
    "ARM_x_val_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_val], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_val[0].keys()})\n",
    "\n",
    "ARM_x_test_tokenized = dict_lists_to_list_of_dicts({\n",
    "    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x_test], padding=True, truncation=True, return_tensors='pt'))\n",
    "for key in ARM_x_test[0].keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1 prende le due componenti e le analizza indipendentemente per poi dare un output\n",
    "\n",
    "Molel2 concatena componente 1 e componente 2 per analizzarle assieme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")#\"FacebookAI/roberta-base\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size * 2, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        _, encoded_input_1 = self.encoder(**input[\"e1\"], return_dict = False)\n",
    "        _, encoded_input_2 = self.encoder(**input[\"e2\"], return_dict = False)\n",
    "        encoded_input = self.dropout(torch.cat((encoded_input_1, encoded_input_2), dim=1))\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "    \n",
    "class Model2(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = .3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = {key: torch.cat((input[\"e1\"][key], input[\"e2\"][key]), dim=1) for key in input[\"e1\"].keys()}\n",
    "        _, encoded_input = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "    \n",
    "        return self.output_layer(encoded_input)\n",
    "\n",
    "model1 = Model1(2)\n",
    "model2 = Model2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARM_x_tokenized = dict_lists_to_list_of_dicts({\n",
    "#    key : dict_lists_to_list_of_dicts(tokenizer([v[key] for v in ARM_x], padding=True, truncation=True, return_tensors='pt'))\n",
    "#for key in ARM_x[0].keys()})\n",
    "#train_dataloader, validation_dataloader, test_dataloader = get_dataloader(ARM_x_tokenized, ARM_y, 8, [9])\n",
    "batch_size = 8\n",
    "train_dataloader, validation_dataloader, test_dataloader = (DataLoader(Dataset(ARM_x_train_tokenized, ARM_y_train), batch_size = batch_size, shuffle = True),\n",
    "                                                            DataLoader(Dataset(ARM_x_val_tokenized, ARM_y_val),batch_size = batch_size, shuffle = False), \n",
    "                                                            DataLoader(Dataset(ARM_x_test_tokenized, ARM_y_test),batch_size = batch_size, shuffle = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.442 - validation loss:      0.534                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.810 - validation accuracy:      0.726\n",
      "EPOCH 1 training f1_score:      0.786 - validation f1_score:      0.634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.408 - validation loss:      0.546                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.825 - validation accuracy:      0.723\n",
      "EPOCH 2 training f1_score:      0.803 - validation f1_score:      0.623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.376 - validation loss:      0.557                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.844 - validation accuracy:      0.725\n",
      "EPOCH 3 training f1_score:      0.826 - validation f1_score:      0.634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.350 - validation loss:      0.561                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.861 - validation accuracy:      0.716\n",
      "EPOCH 4 training f1_score:      0.840 - validation f1_score:      0.619\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.325 - validation loss:      0.587                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.869 - validation accuracy:      0.713\n",
      "EPOCH 5 training f1_score:      0.853 - validation f1_score:      0.611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model1.train_network(train_dataloader, \n",
    "                    validation_dataloader, \n",
    "                    torch.optim.Adam, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-6,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 training loss:      0.565 - validation loss:      0.659                                                                                                                                                                                                                                                      \n",
      "EPOCH 1 training accuracy:      0.730 - validation accuracy:      0.640\n",
      "EPOCH 1 training f1_score:      0.702 - validation f1_score:      0.527\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 2 training loss:      0.526 - validation loss:      0.624                                                                                                                                                                                                                                                      \n",
      "EPOCH 2 training accuracy:      0.762 - validation accuracy:      0.670\n",
      "EPOCH 2 training f1_score:      0.732 - validation f1_score:      0.559\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 3 training loss:      0.490 - validation loss:      0.622                                                                                                                                                                                                                                                      \n",
      "EPOCH 3 training accuracy:      0.784 - validation accuracy:      0.672\n",
      "EPOCH 3 training f1_score:      0.757 - validation f1_score:      0.566\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 4 training loss:      0.459 - validation loss:      0.596                                                                                                                                                                                                                                                      \n",
      "EPOCH 4 training accuracy:      0.799 - validation accuracy:      0.701\n",
      "EPOCH 4 training f1_score:      0.771 - validation f1_score:      0.600\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "EPOCH 5 training loss:      0.423 - validation loss:      0.611                                                                                                                                                                                                                                                      \n",
      "EPOCH 5 training accuracy:      0.818 - validation accuracy:      0.690\n",
      "EPOCH 5 training f1_score:      0.795 - validation f1_score:      0.584\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   model2.train_network(train_dataloader, #rispetto a model1 sembra avere risultati simili, ma ci mette più tempo\n",
    "                    validation_dataloader, \n",
    "                    torch.optim.Adam, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: torch.max(x, -1)[1].view(-1).cpu(), \n",
    "                    metrics={\n",
    "                     \"accuracy\": accuracy_score, \n",
    "                     \"f1_score\": lambda y_true, y_pred: f1_score(y_true, y_pred, average=\"macro\")},\n",
    "                    learning_rate=1e-6,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Premise/Conclusion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\n",
      "1607\n",
      "\n",
      "    There are:\n",
      "        - 1607 clauses\n",
      "        - 1279 premises\n",
      "        - 451 conclusions\n",
      "        \n",
      "validation set:\n",
      "199\n",
      "\n",
      "    There are:\n",
      "        - 199 clauses\n",
      "        - 157 premises\n",
      "        - 62 conclusions\n",
      "        \n",
      "test set:\n",
      "561\n",
      "\n",
      "    There are:\n",
      "        - 561 clauses\n",
      "        - 441 premises\n",
      "        - 155 conclusions\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "train_cases, validation_cases, test_cases = cases[-1]\n",
    "def get_pcr_dataloader(cases, tokenizer, batch_size=16, shuffle=False, verbose=True):\n",
    "    PCR_x = []\n",
    "    PCR_y = []\n",
    "    argdict = {}\n",
    "    for case in cases:\n",
    "        n_clauses = case[\"n_clauses\"]\n",
    "        clauses = case[\"all_clauses\"]\n",
    "        for argument in case[\"arguments\"]:\n",
    "            if not clauses[argument[\"conclusion\"]] in argdict:\n",
    "                argdict[clauses[argument[\"conclusion\"]]] = [0.,1.]\n",
    "            else:\n",
    "                argdict[clauses[argument[\"conclusion\"]]][1] = 1.\n",
    "\n",
    "            for premise in argument[\"premises\"]:\n",
    "                if not clauses[premise] in argdict:\n",
    "                    argdict[clauses[premise]] = [1.,0.]\n",
    "                else:\n",
    "                    argdict[clauses[premise]][0] = 1.\n",
    "\n",
    "    items = list(argdict.items())\n",
    "    print(len(items))\n",
    "    for item in items:\n",
    "        PCR_x.append(item[0])\n",
    "        PCR_y.append(torch.tensor(item[1]))\n",
    "\n",
    "\n",
    "    PCR_x_tokenized = dict_lists_to_list_of_dicts(tokenizer(PCR_x, padding=True, truncation=True, return_tensors='pt'))\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "    There are:\n",
    "        - {len(PCR_x)} clauses\n",
    "        - {len([a for a in PCR_y if a[0] == 1.])} premises\n",
    "        - {len([a for a in PCR_y if a[1] == 1.])} conclusions\n",
    "        \"\"\")\n",
    "    \n",
    "    dataset = Dataset(PCR_x_tokenized, PCR_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print(\"train set:\")\n",
    "PCR_train_dataloader = get_pcr_dataloader(train_cases, tokenizer, shuffle=True, batch_size=8)\n",
    "print(\"validation set:\")\n",
    "PCR_validation_dataloader = get_pcr_dataloader(validation_cases, tokenizer, batch_size=8)\n",
    "print(\"test set:\")\n",
    "PCR_test_dataloader = get_pcr_dataloader(test_cases, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [{'input_ids': tensor([[ 101, 1067,  245,  ...,    0,    0,    0],\n",
       "           [ 101,  226,  246,  ...,    0,    0,    0],\n",
       "           [ 101,  221,  145,  ...,    0,    0,    0],\n",
       "           ...,\n",
       "           [ 101,  809, 1184,  ...,    0,    0,    0],\n",
       "           [ 101,  233,  223,  ...,    0,    0,    0],\n",
       "           [ 101,  207,  389,  ...,    0,    0,    0]]),\n",
       "   'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0],\n",
       "           [0, 0, 0,  ..., 0, 0, 0]]),\n",
       "   'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           ...,\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0],\n",
       "           [1, 1, 1,  ..., 0, 0, 0]])},\n",
       "  tensor([[1., 0.],\n",
       "          [1., 0.],\n",
       "          [0., 1.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [1., 0.],\n",
       "          [0., 1.]])])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(PCR_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPCR(NeuralNetwork):\n",
    "    def __init__(self, out_features:int, dropout:float = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.output_layer = nn.Linear(self.encoder.config.hidden_size, out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        encoded_input, _ = self.encoder(**input, return_dict = False)\n",
    "        encoded_input = self.dropout(encoded_input)\n",
    "        encoded_input = torch.nn.functional.avg_pool1d(\n",
    "            encoded_input.permute(0, 2, 1), \n",
    "            kernel_size=encoded_input.size(1)\n",
    "        ).squeeze(2)\n",
    "        if len(encoded_input.size()) != 2:\n",
    "            print(encoded_input.size())\n",
    "        return self.output_layer(encoded_input).float()\n",
    "\n",
    "    def freeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    def unfreeze_bert(self):\n",
    "        for param in self.encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "modelPCR = ModelPCR(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data, validation_data \u001b[38;5;241m=\u001b[39m   \u001b[43mmodelPCR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPCR_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mPCR_validation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moutput_extraction_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m#\"accuracy\": accuracy_score, \u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1_score_premise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alfio\\Desktop\\NLP_project\\neuralNetwork.py:95\u001b[0m, in \u001b[0;36mNeuralNetwork.train_network\u001b[1;34m(self, train_loader, validation_loader, optimizer, loss_function, learning_rate, epochs, batch_size, device, output_extraction_function, metrics, in_between_epochs, verbose, automatically_handle_gpu_memory)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     94\u001b[0m   loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:10.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m---> 95\u001b[0m   str_metrics \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{:10.3f}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     96\u001b[0m   str_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     97\u001b[0m   stdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mbatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----- loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ----- \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_metrics[key]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mkey\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstr_metrics\u001b[38;5;241m.\u001b[39mkeys()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alfio\\Desktop\\NLP_project\\neuralNetwork.py:95\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     94\u001b[0m   loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:10.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(loss\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m---> 95\u001b[0m   str_metrics \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:10.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_classes\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     96\u001b[0m   str_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     97\u001b[0m   stdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mbatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----- loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ----- \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_metrics[key]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mkey\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mstr_metrics\u001b[38;5;241m.\u001b[39mkeys()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m train_data, validation_data \u001b[38;5;241m=\u001b[39m   modelPCR\u001b[38;5;241m.\u001b[39mtrain_network(PCR_train_dataloader, \n\u001b[0;32m      2\u001b[0m                     PCR_validation_dataloader, \n\u001b[0;32m      3\u001b[0m                     torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam, \n\u001b[0;32m      4\u001b[0m                     loss_function\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(),\n\u001b[0;32m      5\u001b[0m                     device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[0;32m      6\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      7\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      8\u001b[0m                     output_extraction_function\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: (x\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m0\u001b[39m],x\u001b[38;5;241m.\u001b[39mcpu()[\u001b[38;5;241m1\u001b[39m]), \n\u001b[0;32m      9\u001b[0m                     metrics\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     10\u001b[0m                      \u001b[38;5;66;03m#\"accuracy\": accuracy_score, \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score_premise\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m},\n\u001b[0;32m     12\u001b[0m                     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m,\n\u001b[0;32m     13\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1239\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1098\u001b[0m ):\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1413\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1252\u001b[0m     {\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1281\u001b[0m ):\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03m    0.38...\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1724\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m \n\u001b[0;32m   1568\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1724\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1501\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1501\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:311\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:167\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    165\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:1030\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "train_data, validation_data =   modelPCR.train_network(PCR_train_dataloader, \n",
    "                    PCR_validation_dataloader, \n",
    "                    torch.optim.Adam, \n",
    "                    loss_function=nn.CrossEntropyLoss(),\n",
    "                    device=device, \n",
    "                    batch_size=32,\n",
    "                    verbose=True, \n",
    "                    output_extraction_function= lambda x: (x.cpu()[0],x.cpu()[1]), \n",
    "                    metrics={\n",
    "                     #\"accuracy\": accuracy_score, \n",
    "                     \"f1_score_premise\": lambda y_true, y_pred: f1_score(y_true[0], y_pred[0], average=\"macro\")},\n",
    "                    learning_rate=1e-6,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
